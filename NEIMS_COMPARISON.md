# NEIMS vs BitSpec 比較分析

## 概要

NEIMS（Neural EI-MS, 2019）は、EI-MS予測で0.84のWeighted Cosine Similarityを達成した最先端モデル。BitSpecとの比較を通じて改善点を特定する。

## 性能比較

⚠️ **重要な注意**: NEIMS と BitSpec は**異なる評価指標**を使用していました！

### 評価指標の違い

| モデル | 評価指標 | 値 | 説明 |
|--------|---------|-----|------|
| **NEIMS** | **Weighted Cosine Similarity (WCS)** | 0.84 | 強度とm/z値に重み付け |
| **BitSpec（旧）** | **Cosine Similarity（非重み付き）** | 0.56 | 通常のコサイン類似度 |
| **BitSpec（新）** | **両方を報告** | - | 公平な比較のため |

### Weighted Cosine Similarity (WCS) とは

```python
# 非重み付き（BitSpec旧実装）
cosine_sim = dot(pred, true) / (norm(pred) * norm(true))

# 重み付き（NEIMS、BitSpec新実装）
weighted_pred = intensity^a * mz^b  # a=0.5-0.6, b=1.3-3.0
weighted_true = intensity^a * mz^b
WCS = dot(weighted_pred, weighted_true) / (norm(weighted_pred) * norm(weighted_true))
```

**重み付けパラメータ**:
- **NIST標準**: (0.6, 3.0)
- **Stein & Scott**: (0.5, 3.0)
- **Kim et al.**: (0.53, 1.3)

**なぜ重み付けが必要か**:
EI-MSでは、m/z値が大きくなると断片イオンの強度が小さくなる傾向がある。重み付けなしでは、重要な高m/z領域のピークの貢献度が低くなってしまう。

### 性能比較（修正版）

| 指標 | NEIMS (2019) | BitSpec (現状) | BitSpec (目標) |
|------|--------------|----------------|---------------|
| **WCS (Stein & Scott)** | 0.84 (NIST17内) | **要再評価** | 0.70-0.80 |
| **Cosine Similarity** | - | 0.56 | - |
| **Training Data** | 約30万スペクトル | 30万化合物 | 同じ |
| **Recall@1** | 86% | - | - |
| **Recall@10** | 91.8% | - | - |
| **予測速度** | 5ms/分子 | - | - |

**重要**:
- Training set外ではNEIMSも0.20まで低下
- **現在の0.56は非重み付き値のため、NEIMSと直接比較不可**
- **WCSで再評価する必要がある**

## アーキテクチャ比較

### NEIMS (MLP-based)
```python
Input: ECFP (Extended Circular Fingerprints)
  - 4096次元ビットベクトル
  - radius=2
  - 分子のサブグラフ出現回数をカウント

Architecture: Multilayer Perceptron
  - 複数の全結合層
  - Bidirectional prediction with gating
  - 物理現象を考慮した調整層

Output: 多次元回帰
  - 各m/z binの強度を直接予測
  - 物理的制約の後処理

Training: 約30万EI-MS (NIST 2017)
```

### BitSpec (GNN-based)
```python
Input: Molecular Graph
  - Node features: 48次元（原子特徴）
  - Edge features: 6次元（結合特徴）
  - より詳細な構造情報

Architecture: Graph Convolutional Network
  - 5層のGCN
  - hidden_dim: 256
  - Mean pooling
  - Spectrum predictor (3層MLP)

Output: Sigmoid出力
  - 0-1範囲の正規化強度

Training: 30万化合物 (NIST17)
  - 事前学習 (PCQM4Mv2)
  - ファインチューニング
```

## 長所・短所比較

### NEIMS
**長所**:
- ✅ シンプルで高速（5ms/分子）
- ✅ 実績のあるECFP特徴量
- ✅ 物理現象を考慮した設計
- ✅ Bidirectional predictionで精度向上

**短所**:
- ❌ グラフ構造を直接利用しない
- ❌ Training set外で性能大幅低下
- ❌ ECFPは固定長・情報損失あり

### BitSpec (現状)
**長所**:
- ✅ グラフ構造を直接学習
- ✅ より豊富な構造情報
- ✅ 事前学習による汎化性能
- ✅ 理論上の表現力が高い

**短所**:
- ❌ 現在は最適化不足（0.56）
- ❌ GNNは学習が難しい
- ❌ 学習速度が遅い可能性
- ❌ 物理制約が不十分

## なぜBitSpecが0.56に留まっているか

### 0. **評価指標の違い**（今回実装）⚠️
- **非重み付きコサイン類似度を使用していた**
- NEIMSはWeighted Cosine Similarity (WCS)を使用
- **直接比較は不可能**
- → WCS実装により公平な比較が可能に

### 1. **最適化の問題**（今回解決）
- Warmup なし → OneCycleLRで解決
- 小バッチ（64） → 実効256で解決
- Early stopping → SWAで解決
- 学習率停滞 → ReduceLROnPlateauで解決

### 2. **アーキテクチャの問題**（次フェーズ）
- hidden_dim 256 → 512に増やせる
- Pooling: mean → attention検討
- Spectrum predictor: より深く

### 3. **NEIMSの特殊な工夫**（長期課題）
- Bidirectional prediction
- Physical adjustments
- Gating mechanisms

## 改善ロードマップ

### Phase 1: 最適化（実装済み）🎯
**目標**: Cosine Similarity 0.70-0.75

**実装内容**:
```yaml
- OneCycleLR (30% warmup)
- Gradient accumulation (実効256)
- SWA (epoch 30-50)
- ReduceLROnPlateau
- persistent_workers + prefetch
```

**期待される改善**: +0.14～0.19
**根拠**: 最新の学習率スケジューリング研究

### Phase 2: アーキテクチャ改善
**目標**: Cosine Similarity 0.75-0.80

**実装案**:
```yaml
model:
  hidden_dim: 512        # 256 → 512
  num_layers: 6          # 5 → 6
  pooling: "attention"   # mean → attention

spectrum_predictor:
  layers: 4              # 3 → 4
  intermediate_dim: 1024 # 512 → 1024
```

**期待される改善**: +0.05～0.08

### Phase 3: NEIMS級の工夫
**目標**: Cosine Similarity 0.80-0.85

**実装案**:
1. **Bidirectional Prediction**
   - 前向き・後ろ向き予測の組み合わせ
   - Gating mechanismで統合

2. **Physical Constraints**
   - 分子量制約
   - Isotope patternの考慮
   - 保存則の組み込み

3. **Ensemble Methods**
   - 複数モデルの予測統合
   - Bootstrap aggregating

4. **Advanced Loss Function**
   - Weighted cosine similarity直接最適化
   - Spectral angle loss
   - Peak-aware loss

**期待される改善**: +0.05～0.10

## NEIMS超えは可能か？

### GNNの理論的優位性

**GNNの利点**:
1. グラフ構造を直接学習
2. より豊富な化学情報
3. 転移学習の恩恵

**実現の条件**:
- 適切な最適化（Phase 1） ← **今実装**
- アーキテクチャ改善（Phase 2）
- 特殊な工夫（Phase 3）

### 現実的な予測

| Phase | 予測性能 | 確率 |
|-------|---------|------|
| Phase 1完了 | 0.70-0.75 | **90%** |
| Phase 2完了 | 0.75-0.80 | **70%** |
| Phase 3完了 | 0.80-0.85 | **40%** |
| NEIMS超え (0.85+) | 0.85-0.90 | **15%** |

### NEIMSを超えるには

**必要な要素**:
1. ✅ Phase 1-3の完全実装
2. ✅ より大規模な事前学習
3. ✅ データ拡張
4. ✅ ハイパーパラメータチューニング
5. ✅ アンサンブル

## 次のステップ

### 即座に実行
```bash
# Phase 1の最適化を実行
python scripts/finetune.py --config config_pretrain.yaml
```

### Phase 1完了後
1. 性能評価（0.70-0.75達成を確認）
2. Phase 2の設計開始
3. NEIMSの論文を詳細に分析
4. Bidirectional predictionの実装検討

## 参考文献

1. NEIMS (2019): "Rapid Prediction of Electron–Ionization Mass Spectrometry Using Neural Networks", ACS Central Science
   - https://pubs.acs.org/doi/10.1021/acscentsci.9b00085

2. NEIMS GitHub: https://github.com/brain-research/deep-molecular-massspec

3. FormulaNet/RASSP: State-of-the-art forward prediction (92.9% weighted dot product)

4. Recent GNN work: "Prediction of electron ionization mass spectra based on graph convolutional networks" (2022)

## 結論

**現状**: 0.56は最適化不足
**短期目標**: 0.70-0.75（Phase 1で達成可能）
**中期目標**: 0.75-0.80（アーキテクチャ改善）
**長期目標**: 0.80-0.85（NEIMS級）
**究極目標**: 0.85+ (NEIMS超え、実現は困難だが理論上可能)

データ規模は同じ（30万）なので、適切な最適化とアーキテクチャ改善により**NEIMS級の性能は実現可能**。

---
作成日: 2025-11-14
最終更新: 2025-11-14
