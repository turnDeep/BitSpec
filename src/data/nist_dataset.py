#!/usr/bin/env python3
# src/data/nist_dataset.py
"""
NEIMS v2.0 NIST EI-MS Dataset Loader

Loads NIST EI-MS data from:
1. NIST17.msp file (if available)
2. MOL files + separate spectrum data

Supports Teacher (GNN+ECFP) and Student (ECFP only) modes.
"""

import os
import pickle
import numpy as np
import torch
from torch.utils.data import Dataset
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from rdkit import Chem
from rdkit.Chem import AllChem, rdFingerprintGenerator
from torch_geometric.data import Data
import logging

# Optional HDF5 support for memory efficient mode
try:
    import h5py
    HDF5_AVAILABLE = True
except ImportError:
    HDF5_AVAILABLE = False

logger = logging.getLogger(__name__)


def parse_msp_file(msp_path: str) -> List[Dict]:
    """
    Parse NIST MSP format file

    Returns:
        entries: List of {name, smiles, mw, formula, spectrum}
    """
    entries = []
    current_entry = {}

    with open(msp_path, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()

            if not line:
                continue

            if line.startswith('Name:'):
                if current_entry:
                    entries.append(current_entry)
                current_entry = {'name': line.split(':', 1)[1].strip()}

            elif line.startswith('MW:'):
                current_entry['mw'] = float(line.split(':')[1].strip())

            elif line.startswith('Formula:'):
                current_entry['formula'] = line.split(':', 1)[1].strip()

            elif line.startswith('SMILES:'):
                current_entry['smiles'] = line.split(':', 1)[1].strip()

            elif line.startswith('ID:'):
                current_entry['id'] = line.split(':', 1)[1].strip()

            elif line.startswith('Num peaks:') or line.startswith('Num Peaks:'):
                current_entry['num_peaks'] = int(line.split(':')[1].strip())
                current_entry['peaks'] = []

            elif 'num_peaks' in current_entry and len(current_entry['peaks']) < current_entry['num_peaks']:
                # Parse peak data: "mz intensity" (one peak per line)
                # Also support semicolon-separated format: "mz intensity; mz intensity; ..."
                if ';' in line:
                    # Semicolon-separated format
                    for peak_str in line.split(';'):
                        if peak_str.strip():
                            parts = peak_str.strip().split()
                            if len(parts) >= 2:
                                mz = float(parts[0])
                                intensity = float(parts[1])
                                current_entry['peaks'].append((mz, intensity))
                else:
                    # One peak per line format
                    parts = line.split()
                    if len(parts) >= 2:
                        try:
                            mz = float(parts[0])
                            intensity = float(parts[1])
                            current_entry['peaks'].append((mz, intensity))
                        except ValueError:
                            pass  # Skip invalid lines

    if current_entry:
        entries.append(current_entry)

    return entries


def peaks_to_spectrum(
    peaks: List[Tuple[float, float]],
    min_mz: int = 1,
    max_mz: int = 1000
) -> np.ndarray:
    """
    Convert peak list to binned spectrum [1, 1000] -> [1000]

    NExtIMS v4.2 Update:
    - Changed from m/z 0-500 (501 dims) to m/z 1-1000 (1000 dims)
    - Better coverage for higher molecular weight compounds
    - 1 Da resolution maintained

    Args:
        peaks: List of (mz, intensity) tuples
        min_mz: Minimum m/z value (default: 1)
        max_mz: Maximum m/z value (default: 1000)

    Returns:
        spectrum: Binned spectrum array [1000] for m/z 1-1000
    """
    spectrum_size = max_mz - min_mz + 1
    spectrum = np.zeros(spectrum_size, dtype=np.float32)

    for mz, intensity in peaks:
        if min_mz <= mz <= max_mz:
            bin_idx = int(round(mz)) - min_mz
            spectrum[bin_idx] = max(spectrum[bin_idx], intensity)

    # Normalize to [0, 1]
    if spectrum.max() > 0:
        spectrum = spectrum / spectrum.max()

    return spectrum


def mol_to_graph(mol: Chem.Mol) -> Data:
    """
    Convert RDKit molecule to PyTorch Geometric graph

    Returns:
        data: PyG Data object with node/edge features
    """
    # Atom features (48-dimensional)
    atom_features = []
    for atom in mol.GetAtoms():
        features = [
            atom.GetAtomicNum(),  # 1
            atom.GetTotalDegree(),  # 1
            atom.GetFormalCharge(),  # 1
            atom.GetTotalNumHs(),  # 1
            atom.GetNumRadicalElectrons(),  # 1
            atom.GetHybridization().real,  # 1
            atom.GetIsAromatic(),  # 1
            atom.IsInRing(),  # 1
        ]
        # One-hot encoding for common properties
        features += [int(atom.GetAtomicNum() == i) for i in range(1, 37)]  # 36
        features += [int(atom.GetTotalDegree() == i) for i in range(7)]  # 7
        atom_features.append(features[:48])  # Ensure 48-dim

    x = torch.tensor(atom_features, dtype=torch.float)

    # Bond features (6-dimensional)
    edge_index = []
    edge_attr = []

    for bond in mol.GetBonds():
        i = bond.GetBeginAtomIdx()
        j = bond.GetEndAtomIdx()

        bond_type = bond.GetBondTypeAsDouble()
        is_conjugated = bond.GetIsConjugated()
        is_aromatic = bond.GetIsAromatic()
        is_in_ring = bond.IsInRing()

        edge_features = [
            bond_type,
            is_conjugated,
            is_aromatic,
            is_in_ring,
            0.0,  # Placeholder
            0.0,  # Placeholder
        ]

        # Bidirectional edges
        edge_index.append([i, j])
        edge_index.append([j, i])
        edge_attr.append(edge_features)
        edge_attr.append(edge_features)

    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    edge_attr = torch.tensor(edge_attr, dtype=torch.float)

    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)


def mol_to_ecfp(mol: Chem.Mol, radius: int = 2, n_bits: int = 4096) -> np.ndarray:
    """
    Generate ECFP4 fingerprint

    Returns:
        ecfp: Binary fingerprint [4096]
    """
    mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=n_bits)
    fp = mfpgen.GetFingerprint(mol)
    return np.array(fp, dtype=np.float32)


def mol_to_count_fp(mol: Chem.Mol, radius: int = 2, n_bits: int = 2048) -> np.ndarray:
    """
    Generate Count fingerprint for Student

    Returns:
        count_fp: Count fingerprint [2048]
    """
    mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=n_bits)
    fp = mfpgen.GetCountFingerprint(mol)
    arr = np.zeros(n_bits, dtype=np.float32)
    for idx, val in fp.GetNonzeroElements().items():
        arr[idx] = min(val, 255)  # Cap at 255
    return arr / 255.0  # Normalize


class NISTDataset(Dataset):
    """
    NIST EI-MS Dataset for NEIMS v2.0

    Supports:
    - MSP file parsing
    - MOL file loading
    - Teacher mode: Graph + ECFP
    - Student mode: ECFP + Count FP
    - Distill mode: Graph + ECFP + Count FP (for knowledge distillation)

    Memory Efficient Mode (IMPLEMENTED):
    Supports lazy loading with HDF5 caching for handling large datasets
    (300k+ molecules) on 32GB RAM systems.

    Features:
    1. HDF5 for on-disk caching (requires h5py library)
    2. Lazy loading: Generates graphs on-the-fly in __getitem__
    3. Minimal memory footprint: Only metadata in RAM
    4. Compatible with gradient accumulation
    5. Automatic fallback to standard mode if h5py unavailable

    Benefits:
    - Memory: 70-100x reduction (5GB vs 17-26GB)
    - Trade-off: ~13% slower training speed
    - Enables 300k+ molecules on 32GB RAM

    Usage:
    Set in config.yaml:
        data:
          memory_efficient_mode:
            enabled: true
            use_lazy_loading: true
            lazy_cache_dir: "data/processed/lazy_cache"
    """

    def __init__(
        self,
        data_config: Dict,
        mode: str = 'teacher',  # 'teacher' or 'student'
        split: str = 'train',   # 'train', 'val', 'test'
        augment: bool = False,
        cache_dir: Optional[str] = None
    ):
        """
        Args:
            data_config: Data configuration from config.yaml
            mode: 'teacher' (GNN+ECFP), 'student' (ECFP+CountFP), or 'distill' (GNN+ECFP+CountFP)
            split: Data split
            augment: Apply data augmentation
            cache_dir: Cache directory for processed data
        """
        self.data_config = data_config
        self.mode = mode
        self.split = split
        self.augment = augment
        self.max_mz = data_config.get('max_mz', 500)

        # Memory efficient mode setup
        mem_config = data_config.get('memory_efficient_mode', {})
        self.memory_efficient = mem_config.get('enabled', False) and HDF5_AVAILABLE
        self.use_lazy_loading = mem_config.get('use_lazy_loading', False) and self.memory_efficient
        self.precompute_graphs = mem_config.get('precompute_graphs', True)

        if self.memory_efficient and not HDF5_AVAILABLE:
            logger.warning("Memory efficient mode requested but h5py not available. Install with: pip install h5py")
            logger.warning("Falling back to standard mode")
            self.memory_efficient = False
            self.use_lazy_loading = False

        if self.memory_efficient:
            logger.info(f"Memory efficient mode ENABLED (lazy_loading={self.use_lazy_loading})")

        # Cache setup
        if cache_dir is None:
            cache_dir = data_config.get('output_dir', 'data/processed')
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # HDF5 cache directory for lazy loading
        if self.use_lazy_loading:
            self.h5_cache_dir = Path(mem_config.get('lazy_cache_dir', 'data/processed/lazy_cache'))
            self.h5_cache_dir.mkdir(parents=True, exist_ok=True)
            self.h5_file = self.h5_cache_dir / f'nist_{split}_{mode}.h5'

        # Load or process data
        if self.use_lazy_loading:
            # Lazy loading: Only load metadata
            metadata_file = self.cache_dir / f'nist_{split}_{mode}_metadata.pkl'
            if metadata_file.exists() and self.h5_file.exists():
                logger.info(f"Loading metadata from {metadata_file}")
                with open(metadata_file, 'rb') as f:
                    self.metadata = pickle.load(f)
                logger.info(f"Lazy loading enabled: {len(self.metadata)} samples (HDF5: {self.h5_file})")
            else:
                logger.info(f"Processing NIST data for lazy loading...")
                self.metadata = self._process_data_lazy()
                with open(metadata_file, 'wb') as f:
                    pickle.dump(self.metadata, f)
                logger.info(f"Metadata cached to {metadata_file}")
        else:
            # Standard mode: Load all data into memory
            cache_file = self.cache_dir / f'nist_{split}_{mode}.pkl'
            if cache_file.exists():
                logger.info(f"Loading cached data from {cache_file}")
                with open(cache_file, 'rb') as f:
                    self.data = pickle.load(f)
            else:
                logger.info(f"Processing NIST data for {split} split...")
                self.data = self._process_data()
                with open(cache_file, 'wb') as f:
                    pickle.dump(self.data, f)
                logger.info(f"Cached data saved to {cache_file}")

            logger.info(f"Loaded {len(self.data)} samples for {split} split")

    def _process_data(self) -> List[Dict]:
        """Process raw NIST data"""
        data = []

        # Try MSP file first
        msp_path = self.data_config.get('nist_msp_path', 'data/NIST17.msp')
        mol_dir = Path(self.data_config.get('mol_files_dir', 'data/mol_files'))

        if os.path.exists(msp_path):
            logger.info(f"Parsing MSP file: {msp_path}")
            entries = parse_msp_file(msp_path)
            logger.info(f"Found {len(entries)} entries in MSP file")

            for entry in entries:
                if 'peaks' not in entry:
                    continue

                # Try to get molecular structure
                mol = None
                smiles = None

                # Option 1: Use SMILES from MSP if available
                if 'smiles' in entry and entry['smiles']:
                    smiles = entry['smiles']
                    mol = Chem.MolFromSmiles(smiles)

                # Option 2: Load from MOL file using ID
                if mol is None and 'id' in entry and mol_dir.exists():
                    mol_file = mol_dir / f"ID{entry['id']}.MOL"
                    if mol_file.exists():
                        mol = Chem.MolFromMolFile(str(mol_file), sanitize=True, removeHs=False)
                        if mol is not None:
                            smiles = Chem.MolToSmiles(mol)

                if mol is None:
                    continue

                spectrum = peaks_to_spectrum(entry['peaks'], self.max_mz)

                sample = {
                    'smiles': smiles,
                    'spectrum': spectrum,
                    'name': entry.get('name', ''),
                }

                # Add features based on mode
                if self.mode == 'teacher':
                    sample['graph'] = mol_to_graph(mol)
                    sample['ecfp'] = mol_to_ecfp(mol)
                elif self.mode == 'student':
                    sample['ecfp'] = mol_to_ecfp(mol)
                    sample['count_fp'] = mol_to_count_fp(mol)
                elif self.mode == 'distill':
                    # Knowledge distillation mode: all features
                    sample['graph'] = mol_to_graph(mol)
                    sample['ecfp'] = mol_to_ecfp(mol)
                    sample['count_fp'] = mol_to_count_fp(mol)

                data.append(sample)

        else:
            # Load from MOL files
            mol_dir = Path(self.data_config.get('mol_files_dir', 'data/mol_files'))
            logger.info(f"Loading MOL files from: {mol_dir}")

            if mol_dir.exists():
                mol_files = sorted(mol_dir.glob('*.MOL'))
                logger.info(f"Found {len(mol_files)} MOL files")

                for mol_file in mol_files:
                    mol = Chem.MolFromMolFile(str(mol_file), sanitize=True, removeHs=False)
                    if mol is None:
                        continue

                    # Generate dummy spectrum (will be replaced with actual data)
                    # In production, load spectrum from paired data file
                    spectrum = np.zeros(self.max_mz + 1, dtype=np.float32)

                    sample = {
                        'smiles': Chem.MolToSmiles(mol),
                        'spectrum': spectrum,
                        'name': mol_file.stem,
                    }

                    if self.mode == 'teacher':
                        sample['graph'] = mol_to_graph(mol)
                        sample['ecfp'] = mol_to_ecfp(mol)
                    else:
                        sample['ecfp'] = mol_to_ecfp(mol)
                        sample['count_fp'] = mol_to_count_fp(mol)

                    data.append(sample)

        # Split data
        np.random.seed(42)
        indices = np.random.permutation(len(data))

        train_ratio = self.data_config.get('train_split', 0.8)
        val_ratio = self.data_config.get('val_split', 0.1)

        n_train = int(len(data) * train_ratio)
        n_val = int(len(data) * val_ratio)

        if self.split == 'train':
            indices = indices[:n_train]
        elif self.split == 'val':
            indices = indices[n_train:n_train + n_val]
        else:  # test
            indices = indices[n_train + n_val:]

        return [data[i] for i in indices]

    def _process_data_lazy(self) -> List[Dict]:
        """
        Process data for lazy loading mode

        Stores only minimal metadata in RAM, saves spectra to HDF5
        """
        metadata = []

        # Try MSP file first
        msp_path = self.data_config.get('nist_msp_path', 'data/NIST17.msp')
        mol_dir = Path(self.data_config.get('mol_files_dir', 'data/mol_files'))

        # Temporary data storage for HDF5
        all_smiles = []
        all_spectra = []
        all_names = []

        if os.path.exists(msp_path):
            logger.info(f"Parsing MSP file: {msp_path}")
            entries = parse_msp_file(msp_path)
            logger.info(f"Found {len(entries)} entries in MSP file")

            for entry in entries:
                if 'peaks' not in entry:
                    continue

                # Try to get molecular structure
                mol = None
                smiles = None

                # Option 1: Use SMILES from MSP if available
                if 'smiles' in entry and entry['smiles']:
                    smiles = entry['smiles']
                    mol = Chem.MolFromSmiles(smiles)

                # Option 2: Load from MOL file using ID
                if mol is None and 'id' in entry and mol_dir.exists():
                    mol_file = mol_dir / f"ID{entry['id']}.MOL"
                    if mol_file.exists():
                        mol = Chem.MolFromMolFile(str(mol_file), sanitize=True, removeHs=False)
                        if mol is not None:
                            smiles = Chem.MolToSmiles(mol)

                if mol is None:
                    continue

                spectrum = peaks_to_spectrum(entry['peaks'], self.max_mz)

                all_smiles.append(smiles)
                all_spectra.append(spectrum)
                all_names.append(entry.get('name', ''))

        else:
            # Fallback to MOL files
            mol_dir = Path(self.data_config.get('mol_files_dir', 'data/mol_files'))
            logger.info(f"Loading MOL files from: {mol_dir}")

            if mol_dir.exists():
                mol_files = sorted(mol_dir.glob('*.MOL'))
                logger.info(f"Found {len(mol_files)} MOL files")

                for mol_file in mol_files:
                    mol = Chem.MolFromMolFile(str(mol_file), sanitize=True, removeHs=False)
                    if mol is None:
                        continue

                    spectrum = np.zeros(self.max_mz + 1, dtype=np.float32)

                    all_smiles.append(Chem.MolToSmiles(mol))
                    all_spectra.append(spectrum)
                    all_names.append(mol_file.stem)

        # Split data
        np.random.seed(42)
        indices = np.random.permutation(len(all_smiles))

        train_ratio = self.data_config.get('train_split', 0.8)
        val_ratio = self.data_config.get('val_split', 0.1)

        n_train = int(len(all_smiles) * train_ratio)
        n_val = int(len(all_smiles) * val_ratio)

        if self.split == 'train':
            split_indices = indices[:n_train]
        elif self.split == 'val':
            split_indices = indices[n_train:n_train + n_val]
        else:  # test
            split_indices = indices[n_train + n_val:]

        # Save spectra to HDF5
        logger.info(f"Saving {len(split_indices)} spectra to HDF5: {self.h5_file}")
        with h5py.File(self.h5_file, 'w') as f:
            # Create datasets
            f.create_dataset('spectra', data=np.array([all_spectra[i] for i in split_indices]))

            # Store SMILES as variable-length strings
            dt = h5py.string_dtype(encoding='utf-8')
            f.create_dataset('smiles', data=[all_smiles[i] for i in split_indices], dtype=dt)
            f.create_dataset('names', data=[all_names[i] for i in split_indices], dtype=dt)

        # Create metadata (only indices and basic info)
        for i, idx in enumerate(split_indices):
            metadata.append({
                'h5_idx': i,  # Index in HDF5 file
                'original_idx': int(idx)  # Original index in full dataset
            })

        logger.info(f"Lazy loading setup complete: {len(metadata)} samples")
        return metadata

    def __len__(self) -> int:
        if self.use_lazy_loading:
            return len(self.metadata)
        else:
            return len(self.data)

    def __getitem__(self, idx: int) -> Dict:
        """
        Returns:
            For teacher: {graph, ecfp, spectrum}
            For student: {ecfp, count_fp, spectrum}
            For distill: {graph, ecfp, count_fp, spectrum}
        """
        if self.use_lazy_loading:
            # Lazy loading: Load from HDF5 and generate features on-the-fly
            metadata = self.metadata[idx]
            h5_idx = metadata['h5_idx']

            # Load from HDF5
            with h5py.File(self.h5_file, 'r') as f:
                smiles = f['smiles'][h5_idx]
                if isinstance(smiles, bytes):
                    smiles = smiles.decode('utf-8')
                spectrum = f['spectra'][h5_idx]

            # Generate features on-the-fly
            mol = Chem.MolFromSmiles(smiles)
            if mol is None:
                # Fallback to simple molecule
                mol = Chem.MolFromSmiles('CCO')
                smiles = 'CCO'

            sample = {
                'smiles': smiles,
                'spectrum': spectrum.copy(),
            }

            # Generate molecular features
            if self.mode == 'teacher':
                sample['graph'] = mol_to_graph(mol)
                sample['ecfp'] = mol_to_ecfp(mol)
            elif self.mode == 'student':
                sample['ecfp'] = mol_to_ecfp(mol)
                sample['count_fp'] = mol_to_count_fp(mol)
            elif self.mode == 'distill':
                # Knowledge distillation mode: all features
                sample['graph'] = mol_to_graph(mol)
                sample['ecfp'] = mol_to_ecfp(mol)
                sample['count_fp'] = mol_to_count_fp(mol)

        else:
            # Standard mode: Data already in memory
            sample = self.data[idx].copy()

        # Data augmentation (if enabled)
        if self.augment and self.split == 'train':
            augmentation_config = self.data_config.get('augmentation', {})

            # 1. Isotope Substitution
            if augmentation_config.get('isotope', {}).get('enabled', False):
                from src.data.augmentation import isotope_substitution
                probability = augmentation_config['isotope'].get('probability', 0.05)
                modified_smiles = isotope_substitution(sample['smiles'], probability)

                # If SMILES changed, regenerate features
                if modified_smiles != sample['smiles']:
                    mol = Chem.MolFromSmiles(modified_smiles)
                    if mol is not None:
                        sample['smiles'] = modified_smiles
                        if self.mode == 'teacher':
                            sample['graph'] = mol_to_graph(mol)
                            sample['ecfp'] = mol_to_ecfp(mol)
                        elif self.mode == 'student':
                            sample['ecfp'] = mol_to_ecfp(mol)
                            sample['count_fp'] = mol_to_count_fp(mol)
                        elif self.mode == 'distill':
                            # Knowledge distillation mode: all features
                            sample['graph'] = mol_to_graph(mol)
                            sample['ecfp'] = mol_to_ecfp(mol)
                            sample['count_fp'] = mol_to_count_fp(mol)

            # 2. Conformer Generation (Teacher pretraining only)
            # Note: Conformer generation is computationally expensive,
            # so we only apply it with low probability or skip for now
            # TODO: Implement conformer-based augmentation for Phase 1 pretraining

            # 3. Label Distribution Smoothing
            if augmentation_config.get('lds', {}).get('enabled', False):
                if np.random.rand() < 0.5:
                    from src.data.augmentation import label_distribution_smoothing
                    sigma = augmentation_config['lds'].get('sigma', 1.5)
                    sample['spectrum'] = label_distribution_smoothing(
                        sample['spectrum'],
                        sigma=sigma
                    )

        # Convert to tensors
        sample['spectrum'] = torch.tensor(sample['spectrum'], dtype=torch.float32)

        if self.mode == 'teacher':
            sample['ecfp'] = torch.tensor(sample['ecfp'], dtype=torch.float32)
        else:
            sample['ecfp'] = torch.tensor(sample['ecfp'], dtype=torch.float32)
            sample['count_fp'] = torch.tensor(sample['count_fp'], dtype=torch.float32)

        return sample


def collate_fn_teacher(batch: List[Dict]) -> Dict:
    """Custom collate for Teacher (handles PyG graphs)"""
    from torch_geometric.data import Batch

    graphs = [sample['graph'] for sample in batch]
    ecfps = torch.stack([sample['ecfp'] for sample in batch])
    spectra = torch.stack([sample['spectrum'] for sample in batch])

    graph_batch = Batch.from_data_list(graphs)

    return {
        'graph': graph_batch,
        'ecfp': ecfps,
        'spectrum': spectra
    }


def collate_fn_student(batch: List[Dict]) -> Dict:
    """Custom collate for Student"""
    ecfps = torch.stack([sample['ecfp'] for sample in batch])
    count_fps = torch.stack([sample['count_fp'] for sample in batch])
    spectra = torch.stack([sample['spectrum'] for sample in batch])

    # Concatenate ECFP + Count FP for Student input
    student_input = torch.cat([ecfps, count_fps], dim=-1)  # [batch, 6144]

    return {
        'input': student_input,
        'ecfp': ecfps,
        'count_fp': count_fps,
        'spectrum': spectra
    }


def collate_fn_distill(batch: List[Dict]) -> Dict:
    """
    Custom collate for Knowledge Distillation

    Combines both Teacher and Student data in a single batch.
    Expects samples to have: graph, ecfp, count_fp, spectrum
    """
    from torch_geometric.data import Batch

    # Teacher data
    graphs = [sample['graph'] for sample in batch]
    graph_batch = Batch.from_data_list(graphs)

    # Common data
    ecfps = torch.stack([sample['ecfp'] for sample in batch])
    count_fps = torch.stack([sample['count_fp'] for sample in batch])
    spectra = torch.stack([sample['spectrum'] for sample in batch])

    # Student input: ECFP + Count FP
    ecfp_count_fp = torch.cat([ecfps, count_fps], dim=-1)  # [batch, 6144]

    return {
        # Teacher inputs
        'graph': graph_batch,
        'ecfp': ecfps,
        # Student inputs
        'ecfp_count_fp': ecfp_count_fp,
        'count_fp': count_fps,
        # Common
        'spectrum': spectra
    }
