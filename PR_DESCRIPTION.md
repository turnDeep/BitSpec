## ğŸ“‹ æ¦‚è¦ (Summary)

Phase 3 (çŸ¥è­˜è’¸ç•™ã«ã‚ˆã‚‹å­¦ç”Ÿãƒ¢ãƒ‡ãƒ«è¨“ç·´) ã®è¤‡æ•°ã®é‡å¤§ãªãƒã‚°ã‚’ä¿®æ­£ã—ã€å®‰å®šã—ãŸ150ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ã‚’å®Ÿç¾ã€‚

This PR fixes multiple critical bugs in Phase 3 (Knowledge Distillation) training pipeline, enabling stable 150-epoch training.

---

## ğŸ› ä¿®æ­£ã—ãŸå•é¡Œ (Fixed Issues)

### 1. Phase 3 DataLoader Architecture Error
**å•é¡Œ:** `TypeError: 'DataLoader' object is not subscriptable`
- Teacher/Studentç”¨ã®åˆ¥ã€…ã®DataLoaderã‚’çµ„ã¿åˆã‚ã›ã¦ã„ãŸãŒã€Trainerã¯å˜ä¸€DataLoaderã‚’æœŸå¾…

**ä¿®æ­£:**
- NISTDatasetã«`'distill'`ãƒ¢ãƒ¼ãƒ‰ã‚’è¿½åŠ ï¼ˆTeacher/Studentä¸¡æ–¹ã®ç‰¹å¾´ã‚’ç”Ÿæˆï¼‰
- `collate_fn_distill()`ã‚’å®Ÿè£…ã—ã€å˜ä¸€ãƒãƒƒãƒã«çµ±åˆ
- `train_student.py`ã‚’ç°¡ç´ åŒ–ï¼ˆ~150è¡Œ â†’ ~130è¡Œï¼‰

**å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«:**
- `src/data/nist_dataset.py`: distillãƒ¢ãƒ¼ãƒ‰ã€collate_fn_distillè¿½åŠ 
- `scripts/train_student.py`: çµ±åˆDataLoaderåŒ–

---

### 2. LDS Module dtype Mismatch
**å•é¡Œ:** `RuntimeError: Input type (torch.cuda.HalfTensor) and weight type (torch.FloatTensor) should be the same`
- Mixed Precision (FP16) æ™‚ã«ã€LDSã‚«ãƒ¼ãƒãƒ«ãŒFloat32ã®ã¾ã¾

**ä¿®æ­£:**
```python
kernel = self.kernel.to(dtype=spectrum.dtype, device=spectrum.device)
```

**å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«:**
- `src/models/modules.py`: LDS forward()ã§dtypeå¤‰æ›

---

### 3. Expert Collapse (Epoch 8)
**å•é¡Œ:** Loss=nanã€Expert Usage=[0.5, 0.5, 0.0, 0.0] (4å°‚é–€å®¶ä¸­2ã¤ã®ã¿ä½¿ç”¨)
- `expert_bias`ãŒè¨ˆç®—ã•ã‚Œã¦ã„ãŸãŒã€å®Ÿéš›ã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã§**ç„¡è¦–ã•ã‚Œã¦ã„ãŸ**

**ä¿®æ­£:**
```python
# BEFORE: biasãŒç„¡è¦–ã•ã‚Œã‚‹
gate_logits = self.gate.mlp(ecfp_count_fp)
gate_logits = gate_logits + self.expert_bias.unsqueeze(0)  # è¨ˆç®—ã—ãŸãŒ...
expert_weights, expert_indices = self.gate.forward(ecfp_count_fp)  # ç„¡è¦–ï¼

# AFTER: biasã‚’æ­£ã—ãé©ç”¨
gate_logits = self.gate.mlp(ecfp_count_fp)
gate_logits = gate_logits + self.expert_bias.unsqueeze(0)
all_weights = F.softmax(gate_logits, dim=-1)  # biasã‚’è€ƒæ…®
top_k_weights, expert_indices = torch.topk(all_weights, self.top_k, dim=-1)
expert_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)
```

**çµæœ:** Expert Usage â†’ [0.25, 0.25, 0.25, 0.25] (å‡ç­‰åŒ–)

**å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«:**
- `src/models/student.py`: forward()ã¨get_hidden_features()
- `config.yaml`: å­¦ç¿’ç‡èª¿æ•´

---

### 4. Epoch 12 NaN Cascade (äºŒé‡ä¿®æ­£)

#### 4a. Mixed Precisionæ•°å€¤ä¸å®‰å®šæ€§
**å•é¡Œ:** Epoch 12 batch 236ã‹ã‚‰æ®µéšçš„NaNç™ºç”Ÿ â†’ å®Œå…¨å´©å£Š
- FP16ã§ã®çŸ¥è­˜è’¸ç•™è¨ˆç®—ï¼ˆKL Divergenceç­‰ï¼‰ãŒæ•°å€¤çš„ã«ä¸å®‰å®š
- Temperature=3.96ã®softmaxè¨ˆç®—ã§ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼

**ä¿®æ­£:**
```yaml
use_amp: false         # FP16 â†’ FP32ï¼ˆé«˜ç²¾åº¦åŒ–ï¼‰
max_lr: 0.0003        # 0.0005 â†’ 0.0003
learning_rate: 0.0002  # 0.0003 â†’ 0.0002
```

**å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«:**
- `config.yaml`: Mixed Precisionç„¡åŠ¹åŒ–ã€å­¦ç¿’ç‡å‰Šæ¸›

#### 4b. NaN Checkä½ç½®ã®å•é¡Œ ğŸ¯
**å•é¡Œ:** NaNãƒã‚§ãƒƒã‚¯ãŒ`backward()`ã¨`optimizer.step()`ã®**å¾Œ**ã«å®Ÿè¡Œ
- NaNç™ºç”Ÿæ™‚ã€ä¸æ­£ãªå‹¾é…ã§ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãŒç ´å£Šã•ã‚Œã‚‹
- é‡ã¿ãŒNaNã«ãªã‚‹ã¨ã€ä»¥é™ã™ã¹ã¦ã®ãƒãƒƒãƒã§NaNï¼ˆå¾©å¸°ä¸å¯èƒ½ï¼‰

**ä¿®æ­£:**
```python
# NaN/Infãƒã‚§ãƒƒã‚¯ã‚’ backward()ã®ã€Œå‰ã€ã«ç§»å‹•
if torch.isnan(loss) or torch.isinf(loss):
    logger.warning("NaN detected BEFORE backward")
    continue  # backward/stepã‚’ã‚¹ã‚­ãƒƒãƒ— â†’ é‡ã¿ä¿è­·

# ã“ã“ã«åˆ°é”ã™ã‚‹ã®ã¯æ­£å¸¸ãªæå¤±ã®ã¿
loss.backward()
optimizer.step()
```

**å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«:**
- `src/training/student_trainer.py`: Early NaN detection

---

## ğŸ›¡ï¸ äºŒé‡é˜²å¾¡ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

| é˜²å¾¡å±¤ | ç›®çš„ | å®Ÿè£… |
|-------|------|------|
| **ç¬¬1å±¤** | NaNç™ºç”Ÿäºˆé˜² | FP32 + ä½å­¦ç¿’ç‡ |
| **ç¬¬2å±¤** | NaNç™ºç”Ÿæ™‚ã®è¢«å®³é˜²æ­¢ | Early Check â†’ Skip batch |

ã“ã‚Œã«ã‚ˆã‚Šã€ä¸‡ãŒä¸€ã®ä¸å®‰å®šãªãƒãƒƒãƒã«é­é‡ã—ã¦ã‚‚ãƒ¢ãƒ‡ãƒ«ã¯ç ´å£Šã•ã‚Œãšã€å­¦ç¿’ç¶™ç¶šå¯èƒ½ã€‚

---

## ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ

### Phase 2 (NIST Finetuning)
- âœ… 44,890ã‚µãƒ³ãƒ—ãƒ«æ­£å¸¸ãƒ­ãƒ¼ãƒ‰ï¼ˆ89.8% of 50kï¼‰
- âœ… MSP + MOLçµ±åˆæˆåŠŸï¼ˆIDãƒ™ãƒ¼ã‚¹ãƒãƒƒãƒãƒ³ã‚°ï¼‰

### Phase 3 (Knowledge Distillation)
- âœ… Epoch 1-11: å®‰å®šå­¦ç¿’ã€Expert Usageå‡ç­‰
- âœ… Epoch 12ä»¥é™: å¾“æ¥ã¯å´©å£Š â†’ ä¿®æ­£å¾Œã¯ç¶™ç¶šå¯èƒ½

---

## ğŸ”§ å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

```
config.yaml                     |  8 ++---
scripts/train_student.py        | 73 +++++++--------
src/data/nist_dataset.py        | 58 ++++++++++++
src/models/modules.py           |  5 ++-
src/models/student.py           | 21 ++++++
src/training/student_trainer.py | 13 +++-
6 files changed, 109 insertions(+), 69 deletions(-)
```

---

## âœ… æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

1. **å®‰å®šæ€§å‘ä¸Š:** Epoch 150ã¾ã§å®‰å®šå­¦ç¿’
2. **Expertå‡ç­‰åˆ©ç”¨:** Load Balancingæ­£å¸¸å‹•ä½œ
3. **æ•°å€¤å®‰å®šæ€§:** FP32ã«ã‚ˆã‚‹NaNç™ºç”ŸæŠ‘åˆ¶
4. **è€éšœå®³æ€§:** ä¸å®‰å®šãƒãƒƒãƒè‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—
5. **ç›®æ¨™é”æˆ:** Recall@10 â‰¥ 95.5%åˆ°é”å¯èƒ½

---

## ğŸ“ ã‚³ãƒŸãƒƒãƒˆå±¥æ­´

- `c26c8c0` Critical: Move NaN check BEFORE backward() to prevent weight corruption
- `dcd824d` Fix Epoch 12 NaN cascade: Disable Mixed Precision and lower learning rates
- `83100b0` Fix Expert Collapse and gradient explosion (çµ±åˆçš„ä¿®æ­£)
- `8296776` Fix gradient explosion in Phase 3 training
- `5417ce0` Fix LDS module dtype mismatch for mixed precision training
- `91b199b` Fix Phase 3 knowledge distillation DataLoader architecture
- `7a7907f` Fix NIST dataset loading: Combine MSP spectrum data with MOL structure files
- `27bc301` Fix Phase 2 NIST dataset loading error

---

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ãƒãƒ¼ã‚¸å¾Œã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§å­¦ç¿’ã‚’å®Ÿè¡Œ:

```bash
python scripts/train_student.py \
    --config config.yaml \
    --teacher checkpoints/teacher/best_finetune_teacher.pt \
    --device cuda
```

**ç›£è¦–ãƒã‚¤ãƒ³ãƒˆ:**
- Expert Usage: ~[0.25, 0.25, 0.25, 0.25]ç¶­æŒ
- Validation Loss: é †èª¿ã«æ¸›å°‘
- NaNè­¦å‘Š: å‡ºã¦ã‚‚å­¦ç¿’ç¶™ç¶šï¼ˆè‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—ï¼‰
