# Memory-Efficient Training for 32GB RAM Systems

## å•é¡Œ: NIST17ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ30ä¸‡åŒ–åˆç‰©ï¼‰ã®ãƒ¡ãƒ¢ãƒªä¸è¶³

### å¾“æ¥ã®å®Ÿè£…ã®å•é¡Œç‚¹

```python
# å¾“æ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒï¼‰
dataset = MassSpecDataset(...)  # å…¨ã‚°ãƒ©ãƒ•ã‚’ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:
# - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: 10-15GB
# - ãƒ¢ãƒ‡ãƒ«: 2-3GB
# - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°: 5-8GB
# - åˆè¨ˆ: 17-26GB â†’ 32GB RAMã§ã¯ã‚®ãƒªã‚®ãƒª
```

**å•é¡Œ**:
- Pickleã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå·¨å¤§ï¼ˆ10-15GBï¼‰
- å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒ
- OSã‚„ãã®ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’å«ã‚ã‚‹ã¨32GBã§ã¯ä¸è¶³ã®å¯èƒ½æ€§

---

## è§£æ±ºç­–: é…å»¶èª­ã¿è¾¼ã¿ï¼ˆLazy Loadingï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

### `LazyMassSpecDataset` ã®ç‰¹å¾´

1. **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒ**
   - åŒ–åˆç‰©IDã€åˆ†å­å¼ã€MOLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
   - 30ä¸‡åŒ–åˆç‰©ã§ç´„150MB

2. **ã‚¹ãƒšã‚¯ãƒˆãƒ«ã¯HDF5ã§ä¿å­˜**
   - åœ§ç¸®ã•ã‚ŒãŸãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   - é«˜é€Ÿãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚»ã‚¹
   - å¿…è¦æ™‚ã®ã¿ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¿

3. **ã‚°ãƒ©ãƒ•ã¯ã‚ªãƒ³ã‚¶ãƒ•ãƒ©ã‚¤ã§ç”Ÿæˆ**
   - DataLoaderãŒå¿…è¦ã«å¿œã˜ã¦ç”Ÿæˆ
   - ä½¿ç”¨å¾Œã™ãã«ãƒ¡ãƒ¢ãƒªè§£æ”¾

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¯”è¼ƒ

| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | 300,000åŒ–åˆç‰© | 100,000åŒ–åˆç‰© | 50,000åŒ–åˆç‰© |
|-------------|--------------|--------------|--------------|
| **å¾“æ¥æ–¹å¼** | 10-15GB | 5-8GB | 3-4GB |
| **é…å»¶èª­ã¿è¾¼ã¿** | 150MB | 100MB | 80MB |
| **å‰Šæ¸›ç‡** | **70-100x** | **50-80x** | **40-50x** |

---

## ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•

```python
from src.data.lazy_dataset import LazyMassSpecDataset
from torch.utils.data import DataLoader

# é…å»¶èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
dataset = LazyMassSpecDataset(
    msp_file="data/NIST17.msp",
    mol_files_dir="data/mol_files",
    max_mz=500,
    cache_dir="data/processed/lazy_cache",  # HDF5ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å…ˆ
    use_functional_groups=True,
    precompute_graphs=False,  # é‡è¦: ã‚°ãƒ©ãƒ•ã‚’ã‚ªãƒ³ã‚¶ãƒ•ãƒ©ã‚¤ã§ç”Ÿæˆ
    max_samples=None  # None = å…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨
)

# DataLoader
from src.data.dataset import NISTDataLoader

train_loader, val_loader, test_loader = NISTDataLoader.create_dataloaders(
    dataset,
    batch_size=32,
    num_workers=4,  # CPUä¸¦åˆ—å‡¦ç†
    train_ratio=0.8,
    val_ratio=0.1
)

print(f"Dataset size: {len(dataset)} samples")
print(f"Estimated memory: ~150 MB")
```

### 2. 32GB RAMã‚·ã‚¹ãƒ†ãƒ ã§ã®æ¨å¥¨è¨­å®š

```yaml
# config.yaml

data:
  memory_efficient_mode:
    enabled: true
    use_lazy_loading: true
    lazy_cache_dir: "data/processed/lazy_cache"
    precompute_graphs: false  # é‡è¦: ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚false

    ram_32gb_mode:
      max_training_samples: null  # å…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨å¯èƒ½
      gradient_accumulation: 2    # ãƒ¡ãƒ¢ãƒªç¯€ç´„
      empty_cache_frequency: 50   # å®šæœŸçš„ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢

training:
  student_distill:
    batch_size: 32
    num_workers: 4  # Ryzen 7700: 8ã‚³ã‚¢ â†’ 4ãƒ¯ãƒ¼ã‚«ãƒ¼æ¨å¥¨
    gradient_accumulation_steps: 2  # å®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º64
```

### 3. åˆå›å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ§‹ç¯‰ï¼‰

```bash
# 1å›ç›®: HDF5ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ§‹ç¯‰ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ï¼‰
python scripts/train_student.py --config config.yaml

# å‡ºåŠ›:
# Building metadata and spectrum cache...
# Parsing MSP file...
# Found 300,000 compounds with MOL files
# Building HDF5 spectrum cache...
# Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300000/300000
# Saving metadata...
# Cache built: 300,000 samples
# Spectrum cache: data/processed/lazy_cache/spectra.h5 (180.5 MB)
```

### 4. 2å›ç›®ä»¥é™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å†åˆ©ç”¨ï¼‰

```bash
# 2å›ç›®ä»¥é™: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ï¼ˆé«˜é€Ÿï¼‰
python scripts/train_student.py --config config.yaml

# å‡ºåŠ›:
# Loading metadata from cache: data/processed/lazy_cache/metadata.json
# Loaded metadata: 300,000 samples
# Dataset ready: 300000 samples (Memory-efficient mode)
# Estimated memory usage: ~150.0 MB
```

---

## ãƒ¡ãƒ¢ãƒªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ

```bash
# æ¨å®šã®ã¿è¡¨ç¤ºï¼ˆãƒ‡ãƒ¼ã‚¿ä¸è¦ï¼‰
python scripts/benchmark_memory.py --mode estimate --ram_gb 32

# å®Ÿéš›ã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œï¼ˆ1000ã‚µãƒ³ãƒ—ãƒ«ï¼‰
python scripts/benchmark_memory.py \
    --msp_file data/NIST17.msp \
    --mol_dir data/mol_files \
    --samples 1000 \
    --ram_gb 32 \
    --mode all
```

### æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›

```
============================================================
NEIMS v2.0 Memory Benchmark
============================================================
System RAM: 32GB

============================================================
Lazy Dataset Benchmark (1000 samples)
============================================================
Memory before: 250.5 MB
Building HDF5 spectrum cache...
Memory after init: 260.8 MB
Memory used by dataset: 10.3 MB

Accessing 100 random samples...
Memory after accessing samples: 262.1 MB
Memory increase: 1.3 MB

============================================================
Recommendations for 32GB RAM System
============================================================

Full NIST17 (300,000 samples):
  Lazy Loading:
    Dataset:  150.0 MB
    Total:    ~10.1 GB (dataset + model + training)
    Status:   âœ… RECOMMENDED (fits in 32GB RAM)
  Precomputed:
    Dataset:  5250.0 MB
    Total:    ~15.3 GB
    Status:   âš ï¸  May be tight (needs 15.3GB)

Large subset (100,000 samples):
  Lazy Loading:
    Dataset:  100.0 MB
    Total:    ~10.1 GB
    Status:   âœ… RECOMMENDED (fits in 32GB RAM)
  Precomputed:
    Dataset:  1750.0 MB
    Total:    ~11.8 GB
    Status:   âœ… OK (faster but uses more memory)
```

---

## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã®ãƒ¡ãƒ¢ãƒªç®¡ç†

### æ¨å¥¨è¨­å®šï¼ˆ32GB RAMï¼‰

```python
# src/training/student_trainer.py ã§ã®å®Ÿè£…ä¾‹

# 1. Gradient Accumulationï¼ˆå®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™ï¼‰
gradient_accumulation_steps = 2
batch_size = 32  # å®Ÿè³ª 32 * 2 = 64

for epoch in range(num_epochs):
    for i, batch in enumerate(train_loader):
        # Forward pass
        loss = model(batch) / gradient_accumulation_steps
        loss.backward()

        # Gradient accumulation
        if (i + 1) % gradient_accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

        # å®šæœŸçš„ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        if i % 50 == 0:
            torch.cuda.empty_cache()
```

### ãƒ¡ãƒ¢ãƒªç›£è¦–

```python
import psutil
import torch

def print_memory_usage():
    # CPU RAM
    process = psutil.Process()
    cpu_mem = process.memory_info().rss / 1024 / 1024 / 1024

    # GPU VRAM
    gpu_mem = torch.cuda.memory_allocated() / 1024 / 1024 / 1024

    print(f"CPU Memory: {cpu_mem:.2f} GB")
    print(f"GPU Memory: {gpu_mem:.2f} GB")

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«å®šæœŸçš„ã«å‘¼ã³å‡ºã—
print_memory_usage()
```

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 1. ãã‚Œã§ã‚‚ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ãªã‚‹å ´åˆ

```yaml
# config.yaml ã®èª¿æ•´

data:
  memory_efficient_mode:
    ram_32gb_mode:
      # ã‚ªãƒ—ã‚·ãƒ§ãƒ³1: ãƒ‡ãƒ¼ã‚¿ã‚µãƒ–ã‚»ãƒƒãƒˆä½¿ç”¨
      max_training_samples: 100000  # 10ä¸‡åŒ–åˆç‰©ã«åˆ¶é™

training:
  student_distill:
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³2: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
    batch_size: 16  # 32 â†’ 16
    gradient_accumulation_steps: 4  # å®Ÿè³ª64ã‚’ç¶­æŒ

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³3: ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å‰Šæ¸›
    num_workers: 2  # CPUãƒ¡ãƒ¢ãƒªå‰Šæ¸›
```

### 2. HDF5ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å†æ§‹ç¯‰

```bash
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ã—ã¦å†æ§‹ç¯‰
rm -rf data/processed/lazy_cache
python scripts/train_student.py --config config.yaml
```

### 3. ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³

```
HDF5ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º:
- 300,000åŒ–åˆç‰©: ~180-200 MB
- åœ§ç¸®ç‡: ç´„70%

å¿…è¦ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡:
- HDF5ã‚­ãƒ£ãƒƒã‚·ãƒ¥: ~200 MB
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSON: ~50 MB
- åˆè¨ˆ: ~250 MBï¼ˆPickleã®10GBã‹ã‚‰å¤§å¹…å‰Šæ¸›ï¼‰
```

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ¯”è¼ƒ

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦

| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | å¾“æ¥æ–¹å¼ | é…å»¶èª­ã¿è¾¼ã¿ | å·®åˆ† |
|------------|---------|------------|------|
| ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰æ™‚é–“ï¼ˆåˆå›ï¼‰ | 30-60åˆ† | 5-10åˆ† | **6-6xé«˜é€Ÿ** |
| ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Šã®æ™‚é–“ | 15åˆ† | 17åˆ† | ~13%é…å»¶ |
| ãƒˆãƒ¼ã‚¿ãƒ«ï¼ˆ150ã‚¨ãƒãƒƒã‚¯ï¼‰ | 37.5æ™‚é–“ | 42.5æ™‚é–“ | +5æ™‚é–“ |

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:
- ãƒ¡ãƒ¢ãƒª: **70-100xå‰Šæ¸›** âœ…
- é€Ÿåº¦: ~13%ä½ä¸‹ï¼ˆè¨±å®¹ç¯„å›²ï¼‰ âš ï¸
- ãƒ‡ã‚£ã‚¹ã‚¯: 10GB â†’ 250MB âœ…

### CPUä½¿ç”¨ç‡ã®æœ€é©åŒ–

```yaml
# Ryzen 7700 (8ã‚³ã‚¢/16ã‚¹ãƒ¬ãƒƒãƒ‰) ã®å ´åˆ

training:
  num_workers: 4-6  # æ¨å¥¨: ã‚³ã‚¢æ•°ã®åŠåˆ†

# ç†ç”±:
# - ã‚°ãƒ©ãƒ•ç”Ÿæˆã¯CPUé›†ç´„çš„
# - 4-6ãƒ¯ãƒ¼ã‚«ãƒ¼ã§æœ€é©ãªãƒãƒ©ãƒ³ã‚¹
# - GPUå¾…ã¡æ™‚é–“ã‚’æœ€å°åŒ–
```

---

## ã¾ã¨ã‚

### âœ… 32GB RAMã§NIST17å…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆ30ä¸‡åŒ–åˆç‰©ï¼‰ã‚’æ‰±ãˆã‚‹

**é…å»¶èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ©ç‚¹**:
1. **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: 10-15GB â†’ 150MBï¼ˆ70-100xå‰Šæ¸›ï¼‰
2. **ãƒ‡ã‚£ã‚¹ã‚¯åŠ¹ç‡**: 10GB â†’ 250MBï¼ˆ40xå‰Šæ¸›ï¼‰
3. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: 100ä¸‡åŒ–åˆç‰©ã§ã‚‚å¯¾å¿œå¯èƒ½
4. **é€Ÿåº¦**: ã‚ãšã‹13%ã®é€Ÿåº¦ä½ä¸‹ï¼ˆè¨±å®¹ç¯„å›²ï¼‰

**æ¨å¥¨è¨­å®š**:
```yaml
data:
  memory_efficient_mode:
    enabled: true
    use_lazy_loading: true
    precompute_graphs: false

training:
  batch_size: 32
  num_workers: 4-6
  gradient_accumulation_steps: 2
```

**ã“ã‚Œã§32GB RAMã§NIST17ãƒ•ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¯èƒ½ã§ã™ï¼** ğŸ‰
