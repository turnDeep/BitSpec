# BDEäº‹å‰å­¦ç¿’å®Ÿè£…ã‚¬ã‚¤ãƒ‰ - æˆ¦ç•¥Aå®Œå…¨ç‰ˆ

## ğŸ“‹ æ¦‚è¦

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€NExtIMS v2.0ã«**BDE (Bond Dissociation Energy) äºˆæ¸¬ã‚¿ã‚¹ã‚¯**ã‚’çµ„ã¿è¾¼ã‚€å®Œå…¨ãªå®Ÿè£…ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚

### **QC-GN2oMS2ã¨ã®æ±ºå®šçš„ãªé•ã„**

| é …ç›® | QC-GN2oMS2 | NExtIMS v2.0 (æˆ¦ç•¥A) |
|------|-----------|---------------------|
| **BDEã®ä½¿ã„æ–¹** | **é™çš„ãªå…¥åŠ›ç‰¹å¾´é‡** | **å‹•çš„ãªå­¦ç¿’ã‚¿ã‚¹ã‚¯** |
| **å®Ÿè£…** | `edge_features = [bond_order, BDE]` | `pretrain_loss = MSE(pred_BDE, target_BDE)` |
| **åˆ©ç‚¹** | å®Ÿè£…ãŒç°¡å˜ | **BDEã®æ§‹é€ çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’** |
| **æ±åŒ–æ€§èƒ½** | BDEãŒæ—¢çŸ¥ã®åˆ†å­ã®ã¿ | **æœªçŸ¥ã®åˆ†å­ã«ã‚‚é©ç”¨å¯èƒ½** |
| **MSç¨®åˆ¥** | Tandem MS ([M+H]+) | **EI-MS (70eV)** |

---

## ğŸ¯ å®Ÿè£…æ¸ˆã¿ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

### âœ… Step 1: BDEGenerator (å®Œäº†)
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/data/bde_generator.py`

```python
from src.data.bde_generator import BDEGenerator

# BDEç”Ÿæˆå™¨ã®åˆæœŸåŒ–
bde_gen = BDEGenerator(
    cache_dir="data/processed/bde_cache",
    use_cache=True,
    bde_min=50.0,   # æ­£è¦åŒ–ç¯„å›²
    bde_max=120.0
)

# åˆ†å­ã®BDEäºˆæ¸¬
mol = Chem.MolFromSmiles("CC(=O)OC1=CC=CC=C1C(=O)O")
bde_dict = bde_gen.predict_bde(mol)
# çµæœ: {0: 85.3, 1: 92.1, ...} (kcal/mol)
```

**æ©Ÿèƒ½**:
- ALFABETå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ (MAE 0.58 kcal/mol)
- ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ALFABETæœªå¯¾å¿œåˆ†å­ç”¨)
- HDF5ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚° (é«˜é€ŸåŒ–)
- BDEæ­£è¦åŒ– ([0, 1]ç¯„å›²)

---

### âœ… Step 2: PCQM4Mv2Dataset (å®Œäº†)
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/data/pcqm4m_dataset.py`

```python
from src.data.pcqm4m_dataset import PCQM4Mv2Dataset

# BDEå›å¸°ã‚¿ã‚¹ã‚¯ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
dataset = PCQM4Mv2Dataset(
    data_config=config['data'],
    split='train',
    pretrain_task='bde',  # NEW: 'bde' or 'bond_masking'
    cache_dir='data/processed'
)

# ãƒ‡ãƒ¼ã‚¿å–å¾—
sample = dataset[0]
# è¿”ã‚Šå€¤:
# {
#     'graph': PyG Data (edge_attr ã« BDE ãªã—),
#     'ecfp': ECFP4 fingerprint,
#     'bde_targets': [num_edges, 1]  # å…¨ã‚¨ãƒƒã‚¸ã®BDEç›®æ¨™å€¤
# }
```

**ä¸»è¦é–¢æ•°**:
- `mol_to_graph_with_bde()`: BDEå›å¸°ç”¨ã‚°ãƒ©ãƒ•ç”Ÿæˆ
- `collate_fn_pretrain()`: BDE/Bond Maskingä¸¡å¯¾å¿œ

---

### âœ… Step 3: TeacherModel BDEäºˆæ¸¬ãƒ˜ãƒƒãƒ‰ (å®Œäº†)
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/models/teacher.py`

```python
class TeacherModel(nn.Module):
    def __init__(self, config):
        ...
        # BDEäºˆæ¸¬ãƒ˜ãƒƒãƒ‰ (NEW)
        self.bde_prediction_head = nn.Sequential(
            nn.Linear(2 * hidden_dim + edge_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1),  # 1 BDEå€¤/ã‚¨ãƒƒã‚¸
            nn.Sigmoid()  # [0, 1]æ­£è¦åŒ–
        )

    def forward(self, graph_data, ecfp, return_bde_predictions=False):
        ...
        if return_bde_predictions:
            # ã‚¨ãƒƒã‚¸ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡è¨ˆç®—
            edge_features = concat([node_i, node_j, edge_attr])
            bde_predictions = self.bde_prediction_head(edge_features)
            return spectrum, bde_predictions
```

**QC-GN2oMS2ã¨ã®é•ã„**:
- QC-GN2oMS2: BDEã‚’å…¥åŠ›ã¨ã—ã¦**ä½¿ç”¨**
- NExtIMS v2.0: BDEã‚’**äºˆæ¸¬** â†’ åŒ–å­¦çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’

---

### âœ… Step 4: TeacherLoss (å®Œäº†)
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/training/losses.py`

```python
class TeacherLoss(nn.Module):
    def __init__(self, lambda_bde=1.0):
        ...
        self.lambda_bde = lambda_bde

    def forward(self, ..., bde_predictions=None, bde_targets=None):
        # BDEå›å¸°æå¤±
        if bde_predictions is not None:
            loss_bde = F.mse_loss(bde_predictions, bde_targets)
            loss = self.lambda_bde * loss_bde

            # MAE monitoring
            mae_bde = F.l1_loss(bde_predictions, bde_targets)

            return loss, {
                'bde_loss': loss_bde.item(),
                'bde_mae': mae_bde.item()
            }
```

**æå¤±é–¢æ•°**:
```
L_pretrain = Î»_bde * MSE(predicted_BDE, target_BDE)

# Phase 1: BDEäº‹å‰å­¦ç¿’
# - å…¨ã‚¨ãƒƒã‚¸ã«ã¤ã„ã¦BDEå›å¸°
# - lambda_bde = 1.0 (BDEæå¤±ã®ã¿)

# Phase 2: NIST17ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# - ã‚¹ãƒšã‚¯ãƒˆãƒ«äºˆæ¸¬ã®ã¿
# - BDEã®çŸ¥è­˜ã¯æ—¢ã«GNNã«çµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹
```

---

## ğŸ”§ Step 5: TeacherTraineræ›´æ–° (è¦å®Ÿè£…)

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/training/teacher_trainer.py`

### **å¤‰æ›´ãŒå¿…è¦ãªç®‡æ‰€**

#### **5.1 __init__ãƒ¡ã‚½ãƒƒãƒ‰**

```python
class TeacherTrainer:
    def __init__(self, model, config, device='cuda', phase='pretrain'):
        ...
        # æå¤±é–¢æ•° (BDEå¯¾å¿œ)
        if phase == 'pretrain':
            pretrain_cfg = config['training']['teacher_pretrain']
            pretrain_task = pretrain_cfg.get('pretrain_task', 'bde')  # NEW

            if pretrain_task == 'bde':
                lambda_bde = pretrain_cfg.get('lambda_bde', 1.0)
                self.criterion = TeacherLoss(lambda_bde=lambda_bde)
            else:  # bond_masking
                lambda_bond = pretrain_cfg.get('lambda_bond', 0.1)
                self.criterion = TeacherLoss(lambda_bond=lambda_bond)

            self.pretrain_task = pretrain_task
        else:
            self.criterion = TeacherLoss()
```

#### **5.2 train_stepãƒ¡ã‚½ãƒƒãƒ‰ (äº‹å‰å­¦ç¿’)**

```python
def train_step(self, batch):
    """
    è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—: BDE/Bond Maskingä¸¡å¯¾å¿œ
    """
    self.model.train()
    self.optimizer.zero_grad()

    graph_data = batch['graph'].to(self.device)
    ecfp = batch['ecfp'].to(self.device)

    # Phase 1: äº‹å‰å­¦ç¿’
    if self.phase == 'pretrain':
        # ã‚¿ã‚¹ã‚¯åˆ¤å®š
        if 'bde_targets' in batch:
            # BDEå›å¸°ã‚¿ã‚¹ã‚¯ (NEW)
            bde_targets = batch['bde_targets'].to(self.device)

            # Forward pass
            with autocast('cuda', enabled=self.use_amp):
                # ãƒ€ãƒŸãƒ¼ã‚¹ãƒšã‚¯ãƒˆãƒ« (äº‹å‰å­¦ç¿’ã§ã¯ã‚¹ãƒšã‚¯ãƒˆãƒ«ä¸è¦)
                dummy_spectrum = torch.zeros(
                    ecfp.size(0), 501,
                    device=self.device
                )

                # BDEäºˆæ¸¬
                _, bde_predictions = self.model(
                    graph_data,
                    ecfp,
                    dropout=True,
                    return_bde_predictions=True  # NEW
                )

                # BDEå›å¸°æå¤±
                loss, loss_dict = self.criterion(
                    dummy_spectrum,  # ç„¡è¦–ã•ã‚Œã‚‹
                    dummy_spectrum,  # ç„¡è¦–ã•ã‚Œã‚‹
                    bde_predictions=bde_predictions,
                    bde_targets=bde_targets
                )

        else:
            # Bond Masking ã‚¿ã‚¹ã‚¯ (original)
            mask_targets = batch['mask_targets'].to(self.device)

            with autocast('cuda', enabled=self.use_amp):
                dummy_spectrum = torch.zeros(ecfp.size(0), 501, device=self.device)
                _, bond_predictions = self.model(
                    graph_data, ecfp, dropout=True,
                    return_bond_predictions=True
                )

                loss, loss_dict = self.criterion(
                    dummy_spectrum, dummy_spectrum,
                    bond_predictions=bond_predictions,
                    bond_targets=mask_targets
                )

    # Phase 2: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    else:
        target_spectrum = batch['spectrum'].to(self.device)

        with autocast('cuda', enabled=self.use_amp):
            predicted_spectrum = self.model(graph_data, ecfp, dropout=True)
            loss, loss_dict = self.criterion(
                predicted_spectrum,
                target_spectrum
            )

    # Backward pass
    if self.use_amp:
        self.scaler.scale(loss).backward()
        if self.gradient_clip > 0:
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.gradient_clip
            )
        self.scaler.step(self.optimizer)
        self.scaler.update()
    else:
        loss.backward()
        if self.gradient_clip > 0:
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.gradient_clip
            )
        self.optimizer.step()

    return loss_dict
```

#### **5.3 validate_stepãƒ¡ã‚½ãƒƒãƒ‰**

```python
def validate_step(self, batch):
    """
    æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—: BDE/Bond Maskingä¸¡å¯¾å¿œ
    """
    self.model.eval()

    graph_data = batch['graph'].to(self.device)
    ecfp = batch['ecfp'].to(self.device)

    with torch.no_grad():
        if self.phase == 'pretrain':
            if 'bde_targets' in batch:
                # BDEå›å¸°ã‚¿ã‚¹ã‚¯
                bde_targets = batch['bde_targets'].to(self.device)
                dummy_spectrum = torch.zeros(ecfp.size(0), 501, device=self.device)

                _, bde_predictions = self.model(
                    graph_data, ecfp,
                    return_bde_predictions=True
                )

                loss, loss_dict = self.criterion(
                    dummy_spectrum, dummy_spectrum,
                    bde_predictions=bde_predictions,
                    bde_targets=bde_targets
                )

            else:
                # Bond Masking ã‚¿ã‚¹ã‚¯
                mask_targets = batch['mask_targets'].to(self.device)
                dummy_spectrum = torch.zeros(ecfp.size(0), 501, device=self.device)

                _, bond_predictions = self.model(
                    graph_data, ecfp,
                    return_bond_predictions=True
                )

                loss, loss_dict = self.criterion(
                    dummy_spectrum, dummy_spectrum,
                    bond_predictions=bond_predictions,
                    bond_targets=mask_targets
                )

        else:
            # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
            target_spectrum = batch['spectrum'].to(self.device)
            predicted_spectrum = self.model(graph_data, ecfp)
            loss, loss_dict = self.criterion(
                predicted_spectrum,
                target_spectrum
            )

    return loss_dict
```

---

## âš™ï¸ Step 6: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°

**ãƒ•ã‚¡ã‚¤ãƒ«**: `config_pretrain.yaml`

```yaml
# NEIMS v2.0 äº‹å‰å­¦ç¿’è¨­å®š (BDE Regression)

data:
  pcqm4mv2_path: 'data/pcqm4mv2'
  output_dir: 'data/processed'
  max_samples: 500000  # ã‚µãƒ–ã‚»ãƒƒãƒˆä½¿ç”¨ (å…¨ä½“: 3.74M)

model:
  teacher:
    gnn:
      use_bond_breaking: true  # BDEäºˆæ¸¬ã«å¿…é ˆ
      hidden_dim: 256
      edge_dim: 128
      num_layers: 8
      dropout: 0.3
    # ... (ä»–ã®è¨­å®šã¯åŒã˜)

training:
  teacher_pretrain:
    # NEW: BDEå›å¸°ã‚¿ã‚¹ã‚¯
    pretrain_task: 'bde'  # 'bde' or 'bond_masking'
    lambda_bde: 1.0  # BDEæå¤±ã®é‡ã¿

    # å­¦ç¿’è¨­å®š
    batch_size: 32
    num_epochs: 50
    learning_rate: 1e-4
    optimizer: 'AdamW'
    weight_decay: 1e-5

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
    scheduler: 'CosineAnnealingWarmRestarts'
    scheduler_t0: 10
    scheduler_tmult: 2

    # æ—©æœŸçµ‚äº†
    early_stopping:
      patience: 20
      min_delta: 0.0001

    # Mixed Precision
    use_amp: true
    gradient_clip: 1.0

  teacher_finetune:
    # Phase 2ã¯BDEä½¿ç”¨ã›ãš (æ—¢ã«GNNã«çµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹)
    batch_size: 32
    num_epochs: 100
    learning_rate: 1e-4
    # ... (æ—¢å­˜è¨­å®š)
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### **æ–¹æ³•1: çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (æ¨å¥¨)**

```bash
# 3æ®µéšã‚’è‡ªå‹•å®Ÿè¡Œ
python scripts/train_pipeline.py --config config_pretrain.yaml

# Phase 1: BDEäº‹å‰å­¦ç¿’ (PCQM4Mv2)
# Phase 2: NIST17ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# Phase 3: Studentè’¸ç•™
```

### **æ–¹æ³•2: å€‹åˆ¥å®Ÿè¡Œ**

```bash
# Phase 1: BDEäº‹å‰å­¦ç¿’ã®ã¿
python scripts/train_teacher.py \
    --config config_pretrain.yaml \
    --phase pretrain

# Phase 2: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
python scripts/train_teacher.py \
    --config config.yaml \
    --phase finetune \
    --pretrained checkpoints/teacher/best_pretrain_teacher.pt
```

---

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹çµæœ

### **Phase 1: BDEäº‹å‰å­¦ç¿’**

```
Epoch 1/50
  Train BDE Loss: 0.0245, MAE: 0.0512 (normalized)
  Val BDE Loss: 0.0198, MAE: 0.0445

Epoch 50/50
  Train BDE Loss: 0.0052, MAE: 0.0158
  Val BDE Loss: 0.0049, MAE: 0.0152

BDE MAE (denormalized): 1.06 kcal/mol
(ALFABET: 0.58 kcal/mol - ç›®æ¨™å€¤)
```

**æ­£å¸¸æ€§ãƒã‚§ãƒƒã‚¯**:
- BDE Loss < 0.01 (50ã‚¨ãƒãƒƒã‚¯å¾Œ)
- BDE MAE < 0.02 (normalized)
- BDE MAE < 1.5 kcal/mol (denormalized)

### **Phase 2: NIST17ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**

```
Epoch 100/100
  Recall@10: 96.2% (+0.7% vs Bond Masking)
  Recall@5: 91.3% (+1.3% vs Bond Masking)
  Cosine Similarity: 0.785 (+3% vs Bond Masking)
```

**æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„**:
- Recall@10: 95.5% â†’ **96.0-96.5%** (+0.5-1.0%)
- Recall@5: 90% â†’ **91-92%** (+1-2%)

---

## ğŸ” ãƒ‡ãƒãƒƒã‚° & ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### **å•é¡Œ1: ALFABETæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**

```bash
# ã‚¨ãƒ©ãƒ¼
ImportError: No module named 'alfabet'

# è§£æ±ºç­–
pip install alfabet

# ã¾ãŸã¯ Fallback ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ¨å®šãŒè‡ªå‹•ä½¿ç”¨ã•ã‚Œã‚‹
# (ç²¾åº¦ã¯ä½ä¸‹: MAE ~3-5 kcal/mol)
```

### **å•é¡Œ2: BDE Lossçˆ†ç™º**

```python
# ç—‡çŠ¶
Epoch 1: BDE Loss = 15.234 (ç•°å¸¸ã«é«˜ã„)

# åŸå› : BDEæ­£è¦åŒ–ç¯„å›²ãŒä¸é©åˆ‡
# è§£æ±ºç­–: bde_min, bde_maxã‚’èª¿æ•´

bde_gen = BDEGenerator(
    bde_min=40.0,   # ã‚ˆã‚Šä½ã
    bde_max=130.0,  # ã‚ˆã‚Šé«˜ã
)
```

### **å•é¡Œ3: ãƒ¡ãƒ¢ãƒªä¸è¶³ (PCQM4Mv2)**

```bash
# ç—‡çŠ¶
CUDA out of memory (VRAM 16GB)

# è§£æ±ºç­–1: ã‚µãƒ–ã‚»ãƒƒãƒˆä½¿ç”¨
config['data']['max_samples'] = 200000  # 20ä¸‡åˆ†å­ã®ã¿

# è§£æ±ºç­–2: ãƒãƒƒãƒã‚µã‚¤ã‚ºå‰Šæ¸›
config['training']['teacher_pretrain']['batch_size'] = 16

# è§£æ±ºç­–3: Gradient Accumulation
config['training']['teacher_pretrain']['gradient_accumulation_steps'] = 2
```

### **å•é¡Œ4: BDEäºˆæ¸¬ç²¾åº¦ãŒä½ã„**

```python
# ç—‡çŠ¶
BDE MAE > 3 kcal/mol (ALFABET: 0.58 kcal/mol)

# åŸå› ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ:
# 1. ALFABETæ­£å¸¸å‹•ä½œç¢ºèª
from src.data.bde_generator import BDEGenerator
bde_gen = BDEGenerator()
print(bde_gen.predictor)  # None ã§ãªã„ã“ã¨ã‚’ç¢ºèª

# 2. ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡ç¢ºèª
# mol_to_graph_with_bde() ã§BDEãŒå…¥åŠ›ç‰¹å¾´é‡ã«å«ã¾ã‚Œã¦ã„ãªã„ã‹ç¢ºèª
# edge_attr ã« BDE ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ã‚¿ã‚¹ã‚¯ãŒç„¡æ„å‘³ã«ãªã‚‹

# 3. ãƒ¢ãƒ‡ãƒ«ã® use_bond_breaking ç¢ºèª
config['model']['teacher']['gnn']['use_bond_breaking'] = True
```

---

## ğŸ“ˆ æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### **è¨ˆç®—æ™‚é–“ (RTX 5070 Ti 16GB)**

| æ®µéš | ãƒ‡ãƒ¼ã‚¿é‡ | æ™‚é–“ (æ¨å®š) | VRAM |
|------|---------|-----------|------|
| BDEå‰è¨ˆç®— | 500Kåˆ†å­ | 1.5æ™‚é–“ | 2GB |
| Phase 1è¨“ç·´ | 50ã‚¨ãƒãƒƒã‚¯ | 18-24æ™‚é–“ | 14GB |
| Phase 2è¨“ç·´ | 100ã‚¨ãƒãƒƒã‚¯ | 12-18æ™‚é–“ | 12GB |
| **åˆè¨ˆ** | - | **32-43æ™‚é–“** | - |

**æœ€é©åŒ–Tips**:
- BDEã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ä½¿ç”¨ â†’ 2å›ç›®ä»¥é™ã¯å³åº§
- Mixed Precision (FP16) â†’ 30%é«˜é€ŸåŒ–
- Gradient Accumulation â†’ VRAMå‰Šæ¸›

---

## âœ… å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [x] **Step 1**: BDEGeneratorä½œæˆ
- [x] **Step 2**: PCQM4Mv2Datasetæ›´æ–°
- [x] **Step 3**: TeacherModel BDEäºˆæ¸¬ãƒ˜ãƒƒãƒ‰è¿½åŠ 
- [x] **Step 4**: TeacherLoss BDEå›å¸°æå¤±è¿½åŠ 
- [ ] **Step 5**: TeacherTrainer BDEå¯¾å¿œ (è¦å®Ÿè£…)
- [ ] **Step 6**: config_pretrain.yamlæ›´æ–°
- [ ] **Step 7**: çµ±åˆãƒ†ã‚¹ãƒˆ

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

**ALFABET**:
- Paper: https://www.nature.com/articles/s41467-020-16201-z
- GitHub: https://github.com/NREL/alfabet
- Dataset: https://figshare.com/articles/dataset/10248932

**QC-GN2oMS2**:
- Paper: https://pubs.acs.org/doi/10.1021/acs.jcim.4c00446
- æ€§èƒ½: Cosine Similarity 0.462 (BDEä½¿ç”¨), 0.437 (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³)

**NExtIMS v2.0**:
- Teacher-Studentè’¸ç•™: è³ªé‡ã‚¹ãƒšã‚¯ãƒˆãƒ«äºˆæ¸¬ã§ä¸–ç•Œåˆ
- BDEäº‹å‰å­¦ç¿’: EI-MSã§åˆã®è©¦ã¿
- æœŸå¾…æ€§èƒ½: Recall@10 96.0-96.5% (NEIMS v1.0: 91.8%)

---

## ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### **çŸ­æœŸ (1-2é€±é–“)**
1. Step 5 (TeacherTrainer) å®Ÿè£…
2. å°è¦æ¨¡ãƒ†ã‚¹ãƒˆ (1ä¸‡åˆ†å­)
3. BDE MAEæ¤œè¨¼ (< 1.5 kcal/mol)

### **ä¸­æœŸ (3-4é€±é–“)**
4. PCQM4Mv2ã‚µãƒ–ã‚»ãƒƒãƒˆ (50ä¸‡åˆ†å­) ã§äº‹å‰å­¦ç¿’
5. NIST17ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
6. Recall@10æ€§èƒ½æ¤œè¨¼ (> 96.0%)

### **é•·æœŸ (6-8é€±é–“)**
7. å…¨PCQM4Mv2 (3.74M) ã§äº‹å‰å­¦ç¿’
8. xTB force constantsè¿½åŠ  (æˆ¦ç•¥B)
9. Motifå¯¾ç…§å­¦ç¿’è¿½åŠ  (æˆ¦ç•¥Cã€æœ€é«˜æ€§èƒ½)

---

## ğŸ’¡ ã¾ã¨ã‚

### **æˆ¦ç•¥Aã®å„ªä½æ€§**

| é …ç›® | è©•ä¾¡ |
|------|------|
| **å®Ÿè£…ã‚³ã‚¹ãƒˆ** | â­â­â­â­ (ä½ã„) |
| **QC-GN2oMS2ã¨ã®å·®åˆ¥åŒ–** | â­â­â­â­ (é«˜ã„) |
| **æ€§èƒ½æ”¹å–„** | â­â­â­â­ (Recall@10 +0.5-1.0%) |
| **æ–°è¦æ€§** | â­â­â­â­ (EI-MSã§åˆ) |
| **è«–æ–‡acceptå¯èƒ½æ€§** | â­â­â­â­ (é«˜ã„) |

### **QC-GN2oMS2ã¨ã®æ±ºå®šçš„ãªé•ã„ (å†æ²)**

```python
# QC-GN2oMS2 (é™çš„BDEä½¿ç”¨)
edge_features = [bond_order, BDE_from_ALFABET]  # BDEã¯å›ºå®šå€¤
model(graph_with_BDE_features)

# NExtIMS v2.0 æˆ¦ç•¥A (å‹•çš„BDEå­¦ç¿’)
pretrain_loss = MSE(predicted_BDE, target_BDE)  # BDEã‚’å­¦ç¿’
# â†’ GNNãŒBDEã®æ§‹é€ çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç²å¾—
# â†’ æ–°ã—ã„åˆ†å­ã«ã‚‚æ±åŒ–å¯èƒ½
# â†’ EI-MSãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ã«æœ€é©
```

**ã“ã‚Œã«ã‚ˆã‚Šã€å˜ãªã‚‹BDEä½¿ç”¨ä»¥ä¸Šã®ä¾¡å€¤ã‚’æä¾›ã—ã¾ã™ã€‚**
