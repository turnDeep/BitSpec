#!/usr/bin/env python3
# scripts/train_pipeline.py
"""
BitSpecçµ±åˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

PCQM4Mv2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ â†’ äº‹å‰å­¦ç¿’ â†’ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° â†’ äºˆæ¸¬
ã‚’ä¸€ã¤ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import torch
import yaml
from pathlib import Path
import logging
import argparse
import sys
import subprocess
from datetime import datetime

sys.path.append(str(Path(__file__).parent.parent))

from src.data.pcqm4mv2_loader import PCQM4Mv2DataLoader
from src.utils.rtx50_compat import setup_rtx50_compatibility

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BitSpecPipeline:
    """BitSpecçµ±åˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self, config_path: str, skip_download: bool = False,
                 skip_pretrain: bool = False, skip_finetune: bool = False,
                 pretrain_subset: int = None):
        """
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            skip_download: PCQM4Mv2ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
            skip_pretrain: äº‹å‰å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—
            skip_finetune: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—
            pretrain_subset: äº‹å‰å­¦ç¿’ã§ä½¿ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        """
        self.config_path = Path(config_path)
        self.skip_download = skip_download
        self.skip_pretrain = skip_pretrain
        self.skip_finetune = skip_finetune
        self.pretrain_subset = pretrain_subset

        # è¨­å®šã®èª­ã¿è¾¼ã¿
        with open(self.config_path, 'r') as f:
            self.config = yaml.safe_load(f)

        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        self.device = setup_rtx50_compatibility()
        logger.info(f"Using device: {self.device}")

        # ãƒ‘ã‚¹ã®è¨­å®š
        self.data_dir = Path(self.config['pretraining']['data_path'])
        self.pretrain_checkpoint_dir = Path(self.config['pretraining']['checkpoint_dir'])
        self.finetune_checkpoint_dir = Path(self.config['finetuning']['checkpoint_dir'])

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.pretrain_checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.finetune_checkpoint_dir.mkdir(parents=True, exist_ok=True)

    def step1_download_pcqm4mv2(self):
        """ã‚¹ãƒ†ãƒƒãƒ—1: PCQM4Mv2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        if self.skip_download:
            logger.info("â­ï¸  Skipping PCQM4Mv2 download (--skip-download)")
            return

        logger.info("=" * 80)
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: PCQM4Mv2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        logger.info("=" * 80)

        try:
            # OGBã‚’ä½¿ç”¨ã—ã¦PCQM4Mv2ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            logger.info("Downloading PCQM4Mv2 dataset via OGB...")
            logger.info("This may take a while (dataset size: ~3.8 million molecules)")

            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–ã—ã¦é«˜é€ŸåŒ–ï¼ˆå­˜åœ¨ç¢ºèªã®ã¿ï¼‰
            _, _, _ = PCQM4Mv2DataLoader.create_dataloaders(
                root=str(self.data_dir),
                batch_size=1,  # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ã¿ãªã®ã§å°ã•ã„ãƒãƒƒãƒã‚µã‚¤ã‚º
                num_workers=0,
                node_feature_dim=self.config['model']['node_features'],
                edge_feature_dim=self.config['model']['edge_features'],
                use_subset=100,  # æœ€åˆã®100ã‚µãƒ³ãƒ—ãƒ«ã ã‘ãƒ­ãƒ¼ãƒ‰ã—ã¦å­˜åœ¨ç¢ºèª
                use_cache=False  # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¢ºèªæ™‚ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¸è¦
            )

            logger.info("âœ“ PCQM4Mv2 dataset downloaded successfully!")

        except Exception as e:
            logger.error(f"âŒ Failed to download PCQM4Mv2: {e}")
            raise

    def step2_pretrain(self):
        """ã‚¹ãƒ†ãƒƒãƒ—2: PCQM4Mv2ã§ã®äº‹å‰å­¦ç¿’"""
        if self.skip_pretrain:
            logger.info("â­ï¸  Skipping pretraining (--skip-pretrain)")
            return

        logger.info("=" * 80)
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—2: PCQM4Mv2äº‹å‰å­¦ç¿’")
        logger.info("=" * 80)

        try:
            # äº‹å‰å­¦ç¿’ç”¨ã®ä¸€æ™‚è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼ˆã‚µãƒ–ã‚»ãƒƒãƒˆæŒ‡å®šãŒã‚ã‚‹å ´åˆï¼‰
            config_to_use = self.config_path
            if self.pretrain_subset is not None:
                logger.info(f"Using subset of {self.pretrain_subset} samples for pretraining")
                temp_config = self.config.copy()
                temp_config['pretraining']['use_subset'] = self.pretrain_subset
                temp_config_path = self.config_path.parent / f"temp_config_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml"
                with open(temp_config_path, 'w') as f:
                    yaml.dump(temp_config, f)
                config_to_use = temp_config_path

            # pretrain.pyã‚’å®Ÿè¡Œ
            pretrain_script = Path(__file__).parent / "pretrain.py"
            cmd = [sys.executable, str(pretrain_script), "--config", str(config_to_use)]

            logger.info(f"Running: {' '.join(cmd)}")
            result = subprocess.run(cmd, check=True)

            if result.returncode == 0:
                logger.info("âœ“ Pretraining completed successfully!")
            else:
                raise RuntimeError(f"Pretraining failed with return code {result.returncode}")

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            if self.pretrain_subset is not None and temp_config_path.exists():
                temp_config_path.unlink()

        except Exception as e:
            logger.error(f"âŒ Pretraining failed: {e}")
            raise

    def step3_finetune(self):
        """ã‚¹ãƒ†ãƒƒãƒ—3: EI-MSã‚¿ã‚¹ã‚¯ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
        if self.skip_finetune:
            logger.info("â­ï¸  Skipping finetuning (--skip-finetune)")
            return

        logger.info("=" * 80)
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—3: EI-MSã‚¿ã‚¹ã‚¯ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
        logger.info("=" * 80)

        try:
            # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ç¢ºèª
            pretrained_backbone = self.pretrain_checkpoint_dir / "pretrained_backbone.pt"
            if not pretrained_backbone.exists() and not self.skip_pretrain:
                logger.warning(f"âš ï¸  Pretrained backbone not found at {pretrained_backbone}")
                logger.warning("Training from scratch instead...")

            # finetune.pyã‚’å®Ÿè¡Œ
            finetune_script = Path(__file__).parent / "finetune.py"
            cmd = [sys.executable, str(finetune_script), "--config", str(self.config_path)]

            logger.info(f"Running: {' '.join(cmd)}")
            result = subprocess.run(cmd, check=True)

            if result.returncode == 0:
                logger.info("âœ“ Finetuning completed successfully!")
            else:
                raise RuntimeError(f"Finetuning failed with return code {result.returncode}")

        except Exception as e:
            logger.error(f"âŒ Finetuning failed: {e}")
            raise

    def step4_summary(self):
        """ã‚¹ãƒ†ãƒƒãƒ—4: ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        logger.info("=" * 80)
        logger.info("ğŸ‰ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!")
        logger.info("=" * 80)

        # ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
        pretrained_backbone = self.pretrain_checkpoint_dir / "pretrained_backbone.pt"
        pretrained_best = self.pretrain_checkpoint_dir / "best_pretrained_model.pt"
        finetuned_best = self.finetune_checkpoint_dir / "best_finetuned_model.pt"

        logger.info("\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        if pretrained_backbone.exists():
            logger.info(f"  âœ“ äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³: {pretrained_backbone}")
        if pretrained_best.exists():
            logger.info(f"  âœ“ äº‹å‰å­¦ç¿’ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {pretrained_best}")
        if finetuned_best.exists():
            logger.info(f"  âœ“ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: {finetuned_best}")

        logger.info("\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        logger.info("  äºˆæ¸¬ã‚’å®Ÿè¡Œã™ã‚‹:")
        logger.info(f"    python scripts/predict.py --checkpoint {finetuned_best} --config {self.config_path} --smiles 'CC(=O)OC1=CC=CC=C1C(=O)O'")

    def run(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’å®Ÿè¡Œ"""
        start_time = datetime.now()
        logger.info("=" * 80)
        logger.info("BitSpecçµ±åˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
        logger.info("=" * 80)
        logger.info(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {self.config_path}")
        logger.info(f"ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        logger.info(f"é–‹å§‹æ™‚åˆ»: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("")

        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            self.step1_download_pcqm4mv2()

            # ã‚¹ãƒ†ãƒƒãƒ—2: äº‹å‰å­¦ç¿’
            self.step2_pretrain()

            # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
            self.step3_finetune()

            # ã‚¹ãƒ†ãƒƒãƒ—4: ã‚µãƒãƒªãƒ¼
            self.step4_summary()

            # å®Ÿè¡Œæ™‚é–“ã®è¨ˆç®—
            end_time = datetime.now()
            elapsed_time = end_time - start_time
            logger.info(f"\nâ±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {elapsed_time}")

        except Exception as e:
            logger.error(f"\nâŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒå¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='BitSpecçµ±åˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰â†’äº‹å‰å­¦ç¿’â†’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
  python scripts/train_pipeline.py --config config_pretrain.yaml

  # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®å ´åˆï¼‰
  python scripts/train_pipeline.py --config config_pretrain.yaml --skip-download

  # äº‹å‰å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ï¼‰
  python scripts/train_pipeline.py --config config_pretrain.yaml --skip-pretrain

  # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼ˆå°ã•ãªã‚µãƒ–ã‚»ãƒƒãƒˆã§äº‹å‰å­¦ç¿’ï¼‰
  python scripts/train_pipeline.py --config config_pretrain.yaml --pretrain-subset 10000

  # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã¿å®Ÿè¡Œ
  python scripts/train_pipeline.py --config config_pretrain.yaml --skip-download --skip-pretrain
        """
    )

    parser.add_argument(
        '--config',
        type=str,
        default='config_pretrain.yaml',
        help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: config_pretrain.yamlï¼‰'
    )

    parser.add_argument(
        '--skip-download',
        action='store_true',
        help='PCQM4Mv2ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—'
    )

    parser.add_argument(
        '--skip-pretrain',
        action='store_true',
        help='äº‹å‰å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ï¼‰'
    )

    parser.add_argument(
        '--skip-finetune',
        action='store_true',
        help='ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—'
    )

    parser.add_argument(
        '--pretrain-subset',
        type=int,
        default=None,
        help='äº‹å‰å­¦ç¿’ã§ä½¿ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ã€ä¾‹: 10000ï¼‰'
    )

    args = parser.parse_args()

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä½œæˆã¨å®Ÿè¡Œ
    pipeline = BitSpecPipeline(
        config_path=args.config,
        skip_download=args.skip_download,
        skip_pretrain=args.skip_pretrain,
        skip_finetune=args.skip_finetune,
        pretrain_subset=args.pretrain_subset
    )

    pipeline.run()


if __name__ == '__main__':
    main()
