# scripts/pretrain.py
"""
PCQM4Mv2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§GCNãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã‚’äº‹å‰å­¦ç¿’ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
import yaml
from pathlib import Path
import logging
from tqdm import tqdm
import wandb
from datetime import datetime
import sys
import time
sys.path.append(str(Path(__file__).parent.parent))

from src.models.gcn_model import GCNMassSpecPredictor, PretrainHead, MultiTaskPretrainHead
from src.data.pcqm4mv2_loader import PCQM4Mv2DataLoader
from src.utils.rtx50_compat import setup_rtx50_compatibility

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PretrainTrainer:
    """äº‹å‰å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼"""

    def __init__(self, config_path: str):
        """
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        # è¨­å®šã®èª­ã¿è¾¼ã¿
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)

        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã¨RTX 50äº’æ›æ€§
        self.device = setup_rtx50_compatibility()
        logger.info(f"Using device: {self.device}")

        # Weights & Biasesã®åˆæœŸåŒ–
        if self.config.get('pretraining', {}).get('use_wandb', False):
            wandb.init(
                project=self.config['pretraining'].get('wandb_project', 'bitspec-pretrain'),
                config=self.config,
                name=f"pretrain_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )

        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
        logger.info("Creating PCQM4Mv2 dataloaders...")
        self.train_loader, self.val_loader, self.test_loader = PCQM4Mv2DataLoader.create_dataloaders(
            root=self.config['pretraining']['data_path'],
            batch_size=self.config['pretraining']['batch_size'],
            num_workers=self.config['pretraining'].get('num_workers', 4),
            node_feature_dim=self.config['model']['node_features'],
            edge_feature_dim=self.config['model']['edge_features'],
            use_subset=self.config['pretraining'].get('use_subset', None),
            prefetch_factor=self.config['pretraining'].get('prefetch_factor', 2),
            use_cache=True  # ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹åŒ–ï¼ˆ2å›ç›®ä»¥é™ã®ã‚¨ãƒãƒƒã‚¯ãŒé«˜é€ŸåŒ–ï¼‰
        )

        # GCNãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®ä½œæˆ
        logger.info("Creating GCN backbone...")
        self.backbone = GCNMassSpecPredictor(
            node_features=self.config['model']['node_features'],
            edge_features=self.config['model']['edge_features'],
            hidden_dim=self.config['model']['hidden_dim'],
            num_layers=self.config['model']['num_layers'],
            spectrum_dim=self.config['model'].get('spectrum_dim', 1000),
            dropout=self.config['model']['dropout'],
            conv_type=self.config['model'].get('gcn', {}).get('conv_type', 'GCNConv'),
            pooling=self.config['model'].get('pooling', 'attention'),
            activation=self.config['model'].get('gcn', {}).get('activation', 'relu'),
            batch_norm=self.config['model'].get('gcn', {}).get('batch_norm', True),
            residual=self.config['model'].get('gcn', {}).get('residual', True)
        ).to(self.device)

        # äº‹å‰å­¦ç¿’ãƒ˜ãƒƒãƒ‰ã®ä½œæˆ
        task = self.config['pretraining'].get('task', 'homo_lumo_gap')
        if task == 'homo_lumo_gap':
            logger.info("Using single-task pretraining (HOMO-LUMO gap)")
            self.pretrain_head = PretrainHead(
                hidden_dim=self.config['model']['hidden_dim'],
                dropout=self.config['model']['dropout']
            ).to(self.device)
        elif task == 'multi_task':
            logger.info("Using multi-task pretraining")
            self.pretrain_head = MultiTaskPretrainHead(
                hidden_dim=self.config['model']['hidden_dim'],
                dropout=self.config['model']['dropout']
            ).to(self.device)
        else:
            raise ValueError(f"Unknown task: {task}")

        self.task = task

        # torch.compileã®é©ç”¨ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰
        # æ³¨æ„: PyTorch Geometricãƒ¢ãƒ‡ãƒ«ã¯torch.compileã¨ç›¸æ€§ãŒæ‚ªã„å ´åˆãŒã‚ã‚‹
        # ç‰¹ã«reduce-overheadãƒ¢ãƒ¼ãƒ‰ã¯åˆå›ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«10-30åˆ†ä»¥ä¸Šã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚‹
        if self.config.get('gpu', {}).get('compile', False):
            compile_mode = self.config.get('gpu', {}).get('compile_mode', 'reduce-overhead')
            logger.info(f"torch.compile is enabled with mode: {compile_mode}")
            logger.warning("âš ï¸  torch.compile with PyTorch Geometric can cause VERY LONG compilation times (10-30+ min)")
            logger.warning("âš ï¸  If training appears frozen, it's likely compiling. Please wait or disable compile in config.")
            logger.info("Starting torch.compile (this may take 10-30 minutes for the first batch)...")

            # ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
            logger.info("Compiling backbone...")
            self.backbone = torch.compile(self.backbone, mode=compile_mode)
            logger.info("âœ“ Backbone compiled")

            # ãƒ˜ãƒƒãƒ‰ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
            logger.info("Compiling pretrain head...")
            self.pretrain_head = torch.compile(self.pretrain_head, mode=compile_mode)
            logger.info("âœ“ Pretrain head compiled")

            logger.info("Note: Actual graph compilation will occur on first forward pass")
        else:
            logger.info("torch.compile is disabled - using eager execution")

        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        self.optimizer = AdamW(
            list(self.backbone.parameters()) + list(self.pretrain_head.parameters()),
            lr=self.config['pretraining']['learning_rate'],
            weight_decay=self.config['pretraining']['weight_decay']
        )

        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
        self.scheduler = CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=self.config['pretraining'].get('scheduler_t0', 10),
            T_mult=self.config['pretraining'].get('scheduler_tmult', 2)
        )

        # Mixed Precision Training
        self.scaler = torch.amp.GradScaler('cuda') if self.config['pretraining'].get('use_amp', True) else None

        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.save_dir = Path(self.config['pretraining'].get('checkpoint_dir', 'checkpoints/pretrain'))
        self.save_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®è¿½è·¡
        self.best_val_loss = float('inf')
        self.best_model_path = None

        # Gradient accumulationè¨­å®š
        self.gradient_accumulation_steps = self.config['pretraining'].get('gradient_accumulation_steps', 1)
        logger.info(f"Gradient accumulation steps: {self.gradient_accumulation_steps}")
        logger.info(f"Effective batch size: {self.config['pretraining']['batch_size'] * self.gradient_accumulation_steps}")

    def train_epoch(self, epoch: int) -> dict:
        """1ã‚¨ãƒãƒƒã‚¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"""
        self.backbone.train()
        self.pretrain_head.train()
        total_loss = 0
        num_batches = 0

        # GPUä½¿ç”¨ç‡ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ç”¨
        batch_times = []
        gpu_utils = []
        start_time = time.time()

        if epoch == 1:
            logger.info("Starting epoch 1...")
            logger.info("Waiting for first batch from dataloader (this may take 10-30 seconds)...")

        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}")
        for batch_idx, (graphs, targets) in enumerate(pbar):
            if epoch == 1 and batch_idx == 0:
                logger.info(f"âœ“ First batch received from dataloader")
            batch_start = time.time()

            # åˆå›ãƒãƒƒãƒã®è©³ç´°ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’è¨˜éŒ²
            if epoch == 1 and batch_idx == 0:
                logger.info("=" * 80)
                logger.info("âš ï¸  FIRST BATCH - Starting processing...")
                logger.info(f"   Batch size: {self.config['pretraining']['batch_size']}")
                logger.info(f"   Num nodes in batch: {graphs.x.shape[0] if hasattr(graphs, 'x') else 'unknown'}")
                logger.info(f"   Pooling type: {self.config['model'].get('pooling', 'unknown')}")
                logger.info("   This may take 30-60 seconds for the first batch.")
                logger.info("=" * 80)
                step_start = time.time()

            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
            if epoch == 1 and batch_idx == 0:
                logger.info(f"   [1/5] Transferring data to GPU...")
            graphs = graphs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)
            if epoch == 1 and batch_idx == 0:
                logger.info(f"   [1/5] âœ“ Data transferred ({time.time() - step_start:.2f}s)")
                step_start = time.time()

            # NaN/Infãƒã‚§ãƒƒã‚¯ - ç„¡åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹å ´åˆã¯ãƒãƒƒãƒå…¨ä½“ã‚’ã‚¹ã‚­ãƒƒãƒ—
            valid_mask = torch.isfinite(targets)
            num_invalid = (~valid_mask).sum().item()

            if num_invalid > 0:
                # ç„¡åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹å ´åˆã¯è­¦å‘Šã‚’å‡ºã—ã¦ãƒãƒƒãƒå…¨ä½“ã‚’ã‚¹ã‚­ãƒƒãƒ—
                logger.warning(f"Batch {batch_idx}: Skipping {num_invalid}/{len(targets)} invalid targets")
                continue

            # å‹¾é…ã‚’ã‚¼ãƒ­åŒ–ï¼ˆgradient accumulationã®æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ã¿ï¼‰
            if batch_idx % self.gradient_accumulation_steps == 0:
                self.optimizer.zero_grad()

            # Mixed Precision Training
            if self.scaler:
                with torch.amp.autocast('cuda'):
                    # GCNã§ã‚°ãƒ©ãƒ•è¡¨ç¾ã‚’æŠ½å‡º
                    if epoch == 1 and batch_idx == 0:
                        logger.info(f"   [2/5] Extracting graph features (forward pass)...")
                    graph_features = self.backbone.extract_graph_features(graphs)
                    if epoch == 1 and batch_idx == 0:
                        logger.info(f"   [2/5] âœ“ Features extracted ({time.time() - step_start:.2f}s)")
                        step_start = time.time()

                    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬
                    if epoch == 1 and batch_idx == 0:
                        logger.info(f"   [3/5] Running prediction head...")
                    if self.task == 'homo_lumo_gap':
                        pred = self.pretrain_head(graph_features).squeeze()

                        # ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                        if pred.shape != targets.shape:
                            logger.warning(f"Batch {batch_idx}: Size mismatch - pred: {pred.shape}, targets: {targets.shape}. Skipping batch.")
                            continue

                        loss = F.mse_loss(pred, targets)
                    else:  # multi_task
                        homo_lumo, dipole, energy = self.pretrain_head(graph_features)
                        homo_lumo_pred = homo_lumo.squeeze()

                        # ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                        if homo_lumo_pred.shape != targets.shape:
                            logger.warning(f"Batch {batch_idx}: Size mismatch - pred: {homo_lumo_pred.shape}, targets: {targets.shape}. Skipping batch.")
                            continue

                        # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æå¤±ï¼ˆã“ã“ã§ã¯HOMO-LUMOã®ã¿ä½¿ç”¨ï¼‰
                        loss = F.mse_loss(homo_lumo_pred, targets)
                    if epoch == 1 and batch_idx == 0:
                        logger.info(f"   [3/5] âœ“ Prediction completed ({time.time() - step_start:.2f}s)")
                        step_start = time.time()

                    # Gradient accumulationã®ãŸã‚ã«lossã‚’ã‚¹ã‚±ãƒ¼ãƒ«
                    loss = loss / self.gradient_accumulation_steps

                # é€†ä¼æ’­
                if epoch == 1 and batch_idx == 0:
                    logger.info(f"   [4/5] Running backward pass...")
                self.scaler.scale(loss).backward()
                if epoch == 1 and batch_idx == 0:
                    logger.info(f"   [4/5] âœ“ Backward completed ({time.time() - step_start:.2f}s)")
                    step_start = time.time()

                # Gradient accumulationã®ã‚¹ãƒ†ãƒƒãƒ—ã«é”ã—ãŸã‚‰ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’æ›´æ–°
                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                    if epoch == 1 and batch_idx == 0:
                        logger.info(f"   [5/5] Updating optimizer...")
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        list(self.backbone.parameters()) + list(self.pretrain_head.parameters()),
                        1.0
                    )
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    if epoch == 1 and batch_idx == 0:
                        logger.info(f"   [5/5] âœ“ Optimizer updated ({time.time() - step_start:.2f}s)")
            else:
                # GCNã§ã‚°ãƒ©ãƒ•è¡¨ç¾ã‚’æŠ½å‡º
                graph_features = self.backbone.extract_graph_features(graphs)

                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬
                if self.task == 'homo_lumo_gap':
                    pred = self.pretrain_head(graph_features).squeeze()

                    # ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                    if pred.shape != targets.shape:
                        logger.warning(f"Batch {batch_idx}: Size mismatch - pred: {pred.shape}, targets: {targets.shape}. Skipping batch.")
                        continue

                    loss = F.mse_loss(pred, targets)
                else:  # multi_task
                    homo_lumo, dipole, energy = self.pretrain_head(graph_features)
                    homo_lumo_pred = homo_lumo.squeeze()

                    # ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                    if homo_lumo_pred.shape != targets.shape:
                        logger.warning(f"Batch {batch_idx}: Size mismatch - pred: {homo_lumo_pred.shape}, targets: {targets.shape}. Skipping batch.")
                        continue

                    loss = F.mse_loss(homo_lumo_pred, targets)

                # Gradient accumulationã®ãŸã‚ã«lossã‚’ã‚¹ã‚±ãƒ¼ãƒ«
                loss = loss / self.gradient_accumulation_steps

                # é€†ä¼æ’­
                loss.backward()

                # Gradient accumulationã®ã‚¹ãƒ†ãƒƒãƒ—ã«é”ã—ãŸã‚‰ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’æ›´æ–°
                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(
                        list(self.backbone.parameters()) + list(self.pretrain_head.parameters()),
                        1.0
                    )
                    self.optimizer.step()

            total_loss += loss.item() * self.gradient_accumulation_steps
            num_batches += 1

            # ãƒãƒƒãƒå‡¦ç†æ™‚é–“ã®è¨˜éŒ²
            batch_time = time.time() - batch_start
            batch_times.append(batch_time)

            # åˆå›ãƒãƒƒãƒå®Œäº†æ™‚ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            if epoch == 1 and batch_idx == 0:
                logger.info("=" * 80)
                logger.info(f"ğŸ‰ FIRST BATCH COMPLETED in {batch_time:.1f}s")
                logger.info(f"   Total time: {batch_time:.1f}s")
                logger.info(f"   Throughput: {self.config['pretraining']['batch_size'] / batch_time:.1f} samples/s")
                logger.info("   Subsequent batches will be faster due to CUDA kernel caching.")
                logger.info("=" * 80)

            # GPUä½¿ç”¨ç‡ã®å–å¾—ï¼ˆ10ãƒãƒƒãƒã”ã¨ï¼‰
            if batch_idx % 10 == 0 and torch.cuda.is_available():
                try:
                    gpu_util = torch.cuda.utilization(self.device)
                    gpu_utils.append(gpu_util)
                except:
                    pass

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æ›´æ–°ï¼ˆGPUä½¿ç”¨ç‡ã‚’è¿½åŠ ï¼‰
            postfix = {'loss': loss.item() * self.gradient_accumulation_steps}
            if gpu_utils:
                postfix['GPU%'] = f"{gpu_utils[-1]}"
            if batch_times:
                postfix['batch/s'] = f"{1.0/batch_time:.2f}"
            pbar.set_postfix(postfix)

            # Weights & Biasesã¸ã®ãƒ­ã‚°
            if self.config.get('pretraining', {}).get('use_wandb', False):
                wandb.log({
                    'train/loss': loss.item(),
                    'train/lr': self.optimizer.param_groups[0]['lr'],
                    'train/batch_time': batch_time,
                    'train/gpu_util': gpu_utils[-1] if gpu_utils else 0,
                    'epoch': epoch
                })

        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0

        # ã‚¨ãƒãƒƒã‚¯çµ±è¨ˆã®å‡ºåŠ›
        if batch_times:
            avg_batch_time = sum(batch_times) / len(batch_times)
            throughput = self.config['pretraining']['batch_size'] / avg_batch_time
            logger.info(f"  Avg batch time: {avg_batch_time:.3f}s, Throughput: {throughput:.1f} samples/s")

        if gpu_utils:
            avg_gpu_util = sum(gpu_utils) / len(gpu_utils)
            logger.info(f"  Avg GPU utilization: {avg_gpu_util:.1f}%")

        return {'loss': avg_loss}

    def validate(self, epoch: int) -> dict:
        """æ¤œè¨¼"""
        self.backbone.eval()
        self.pretrain_head.eval()
        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            for graphs, targets in tqdm(self.val_loader, desc="Validation"):
                graphs = graphs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)

                # NaN/Infãƒã‚§ãƒƒã‚¯ - ç„¡åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹å ´åˆã¯ãƒãƒƒãƒå…¨ä½“ã‚’ã‚¹ã‚­ãƒƒãƒ—
                valid_mask = torch.isfinite(targets)
                num_invalid = (~valid_mask).sum().item()

                if num_invalid > 0:
                    # ç„¡åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹ãƒãƒƒãƒã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue

                # äºˆæ¸¬
                if self.scaler:
                    with torch.amp.autocast('cuda'):
                        graph_features = self.backbone.extract_graph_features(graphs)

                        if self.task == 'homo_lumo_gap':
                            pred = self.pretrain_head(graph_features).squeeze()

                            # ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                            if pred.shape != targets.shape:
                                logger.warning(f"Validation: Size mismatch - pred: {pred.shape}, targets: {targets.shape}. Skipping batch.")
                                continue

                            loss = F.mse_loss(pred, targets)
                        else:  # multi_task
                            homo_lumo, dipole, energy = self.pretrain_head(graph_features)
                            homo_lumo_pred = homo_lumo.squeeze()

                            # ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                            if homo_lumo_pred.shape != targets.shape:
                                logger.warning(f"Validation: Size mismatch - pred: {homo_lumo_pred.shape}, targets: {targets.shape}. Skipping batch.")
                                continue

                            loss = F.mse_loss(homo_lumo_pred, targets)
                else:
                    graph_features = self.backbone.extract_graph_features(graphs)

                    if self.task == 'homo_lumo_gap':
                        pred = self.pretrain_head(graph_features).squeeze()

                        # ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                        if pred.shape != targets.shape:
                            logger.warning(f"Validation: Size mismatch - pred: {pred.shape}, targets: {targets.shape}. Skipping batch.")
                            continue

                        loss = F.mse_loss(pred, targets)
                    else:  # multi_task
                        homo_lumo, dipole, energy = self.pretrain_head(graph_features)
                        homo_lumo_pred = homo_lumo.squeeze()

                        # ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                        if homo_lumo_pred.shape != targets.shape:
                            logger.warning(f"Validation: Size mismatch - pred: {homo_lumo_pred.shape}, targets: {targets.shape}. Skipping batch.")
                            continue

                        loss = F.mse_loss(homo_lumo_pred, targets)

                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        metrics = {'loss': avg_loss}

        # Weights & Biasesã¸ã®ãƒ­ã‚°
        if self.config.get('pretraining', {}).get('use_wandb', False):
            wandb.log({f'val/{k}': v for k, v in metrics.items()})

        return metrics

    def save_checkpoint(self, epoch: int, metrics: dict, is_best: bool = False):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜"""
        checkpoint = {
            'epoch': epoch,
            'backbone_state_dict': self.backbone.state_dict(),
            'pretrain_head_state_dict': self.pretrain_head.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'metrics': metrics,
            'config': self.config
        }

        # å®šæœŸçš„ãªä¿å­˜
        save_interval = self.config['pretraining'].get('save_interval', 10)
        if epoch % save_interval == 0:
            path = self.save_dir / f"checkpoint_epoch_{epoch}.pt"
            torch.save(checkpoint, path)
            logger.info(f"Saved checkpoint: {path}")

        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        if is_best:
            path = self.save_dir / "best_pretrained_model.pt"
            torch.save(checkpoint, path)
            self.best_model_path = path
            logger.info(f"Saved best model: {path}")

            # ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®ã¿ã‚‚ä¿å­˜ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ï¼‰
            backbone_path = self.save_dir / "pretrained_backbone.pt"
            torch.save(self.backbone.state_dict(), backbone_path)
            logger.info(f"Saved backbone weights: {backbone_path}")

    def train(self):
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ"""
        logger.info("Starting pretraining...")
        logger.info(f"Task: {self.task}")
        logger.info(f"Train batches: {len(self.train_loader)}")
        logger.info(f"Val batches: {len(self.val_loader)}")

        num_epochs = self.config['pretraining']['num_epochs']
        for epoch in range(1, num_epochs + 1):
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            train_metrics = self.train_epoch(epoch)
            logger.info(f"Epoch {epoch}/{num_epochs} - Train Loss: {train_metrics['loss']:.4f}")

            # æ¤œè¨¼
            val_metrics = self.validate(epoch)
            logger.info(f"Epoch {epoch}/{num_epochs} - Val Loss: {val_metrics['loss']:.4f}")

            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ã‚¹ãƒ†ãƒƒãƒ—
            self.scheduler.step()

            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜
            is_best = val_metrics['loss'] < self.best_val_loss
            if is_best:
                self.best_val_loss = val_metrics['loss']

            self.save_checkpoint(epoch, val_metrics, is_best)

        logger.info("Pretraining completed!")
        logger.info(f"Best model saved at: {self.best_model_path}")

        # Weights & Biasesã®ã‚¯ãƒ­ãƒ¼ã‚º
        if self.config.get('pretraining', {}).get('use_wandb', False):
            wandb.finish()


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='Pretrain GCN Backbone on PCQM4Mv2')
    parser.add_argument('--config', type=str, default='config_pretrain.yaml',
                        help='Path to config file')
    args = parser.parse_args()

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ä½œæˆã¨å®Ÿè¡Œ
    trainer = PretrainTrainer(args.config)
    trainer.train()


if __name__ == '__main__':
    main()
